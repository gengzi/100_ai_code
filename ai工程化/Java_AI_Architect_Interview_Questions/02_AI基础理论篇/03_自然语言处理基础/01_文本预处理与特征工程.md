# 文本预处理与特征工程

## 题目1: ⭐⭐ 文本预处理的完整流程与最佳实践

**问题描述**:
请详细说明自然语言处理中文本预处理的完整流程，包括分词、去停用词、标准化等步骤，并解释每一步在NLP任务中的重要作用和实现策略。

**答案要点**:
- **文本清洗**: HTML标签、特殊字符、编码处理
- **分词技术**: 中英文分词的差异和实现方法
- **停用词处理**: 停用词表的作用和自定义策略
- **标准化**: 大小写转换、词干提取、词形还原

**核心原理**:
1. 文本预处理是NLP任务的基础，直接影响后续模型性能
2. 不同预处理步骤适用于不同的应用场景
3. 合理的预处理策略能够显著提升模型效果
4. 需要根据具体任务调整预处理流程

**核心代码示例**:
```java
// 文本预处理器
public class TextPreprocessor {
    private final Set<String> stopWords;
    private final Stemmer stemmer;
    private final Tokenizer tokenizer;

    public TextPreprocessor() {
        this.stopWords = loadStopWords();
        this.stemmer = new PorterStemmer();
        this.tokenizer = new Tokenizer();
    }

    public String preprocess(String text) {
        // 1. 文本清洗
        text = cleanText(text);

        // 2. 分词
        List<String> tokens = tokenizer.tokenize(text);

        // 3. 标准化处理
        List<String> processedTokens = new ArrayList<>();
        for (String token : tokens) {
            token = normalizeToken(token);
            if (isValidToken(token)) {
                processedTokens.add(token);
            }
        }

        return String.join(" ", processedTokens);
    }

    private String cleanText(String text) {
        // 移除HTML标签
        text = text.replaceAll("<[^>]+>", "");
        // 移除特殊字符
        text = text.replaceAll("[^a-zA-Z0-9\\s]", "");
        // 处理多余空格
        text = text.replaceAll("\\s+", " ").trim();
        return text;
    }

    private String normalizeToken(String token) {
        // 转小写
        token = token.toLowerCase();
        // 词干提取
        token = stemmer.stem(token);
        return token;
    }

    private boolean isValidToken(String token) {
        return !token.isEmpty() &&
               token.length() > 1 &&
               !stopWords.contains(token);
    }
}
```

---

## 题目2: ⭐⭐⭐ TF-IDF权重计算与特征向量表示

**问题描述**:
请详细解释TF-IDF算法的原理和计算方法，说明其在文本特征提取中的作用，并分析其优缺点及改进方案。

**答案要点**:
- **TF计算**: 词频统计和归一化方法
- **IDF计算**: 逆文档频率的意义和计算公式
- **TF-IDF结合**: 权重计算和特征选择
- **应用场景**: 信息检索、文本分类、聚类等
- **改进方案**: TF-IDF变体和现代替代方案

**核心原理**:
1. TF衡量词在文档中的重要性
2. IDF衡量词在语料库中的稀有程度
3. TF-IDF结合两者得出词的综合权重
4. 高权重词既在文档中频繁出现，在语料库中又相对稀有

**核心代码示例**:
```java
// TF-IDF计算器
public class TFIDFCalculator {
    private final Corpus corpus;
    private final Map<String, Integer> documentFrequencies;
    private final int totalDocuments;

    public TFIDFCalculator(Corpus corpus) {
        this.corpus = corpus;
        this.documentFrequencies = calculateDocumentFrequencies();
        this.totalDocuments = corpus.getDocumentCount();
    }

    public Map<String, Double> calculateTFIDF(Document document) {
        Map<String, Double> tfidfScores = new HashMap<>();
        Map<String, Integer> termFrequencies = calculateTermFrequencies(document);

        for (Map.Entry<String, Integer> entry : termFrequencies.entrySet()) {
            String term = entry.getKey();
            int tf = entry.getValue();
            int df = documentFrequencies.getOrDefault(term, 1);

            double tfScore = 1 + Math.log(tf); // 对数TF
            double idfScore = Math.log((double) totalDocuments / df);

            tfidfScores.put(term, tfScore * idfScore);
        }

        return tfidfScores;
    }

    private Map<String, Integer> calculateTermFrequencies(Document document) {
        Map<String, Integer> frequencies = new HashMap<>();
        List<String> terms = document.getTerms();

        for (String term : terms) {
            frequencies.put(term, frequencies.getOrDefault(term, 0) + 1);
        }

        return frequencies;
    }

    public double[] documentToVector(Document document, List<String> vocabulary) {
        double[] vector = new double[vocabulary.size()];
        Map<String, Double> tfidfScores = calculateTFIDF(document);

        for (int i = 0; i < vocabulary.size(); i++) {
            String term = vocabulary.get(i);
            vector[i] = tfidfScores.getOrDefault(term, 0.0);
        }

        // L2归一化
        return normalizeVector(vector);
    }

    private double[] normalizeVector(double[] vector) {
        double norm = 0;
        for (double val : vector) {
            norm += val * val;
        }
        norm = Math.sqrt(norm);

        if (norm > 0) {
            for (int i = 0; i < vector.length; i++) {
                vector[i] /= norm;
            }
        }

        return vector;
    }
}
```

---

## 题目3: ⭐⭐⭐⭐ 词嵌入技术原理与实现

**问题描述**:
请详细说明词嵌入技术的原理，包括Word2Vec、GloVe等算法的核心思想，并比较它们在语义表示上的优势和差异。

**答案要点**:
- **分布式假设**: 相似上下文中的词具有相似含义
- **Word2Vec**: CBOW和Skip-gram模型的设计原理
- **GloVe**: 全局矩阵分解方法的原理
- **语义表示**: 词向量的语义空间和相似度计算
- **训练优化**: 负采样和层次softmax等技术

**核心原理**:
1. 词嵌入将离散词映射到连续向量空间
2. Word2Vec基于局部上下文窗口训练
3. GloVe结合了全局矩阵分解和局部上下文
4. 词向量在向量空间中的几何关系反映语义关系

**核心代码示例**:
```java
// Skip-gram模型核心训练逻辑
public class SkipGramModel {
    private final int embeddingSize;
    private final int vocabularySize;
    private final double[][] inputEmbeddings;
    private final double[][] outputEmbeddings;
    private final Vocabulary vocabulary;

    public void train(List<String> corpus, int windowSize, int epochs) {
        for (int epoch = 0; epoch < epochs; epoch++) {
            for (String document : corpus) {
                List<String> words = tokenize(document);
                trainOnDocument(words, windowSize);
            }
        }
    }

    private void trainOnDocument(List<String> words, int windowSize) {
        for (int i = 0; i < words.size(); i++) {
            String centerWord = words.get(i);
            int centerIndex = vocabulary.getIndex(centerWord);

            // 获取上下文词
            List<String> contextWords = getContextWords(words, i, windowSize);

            for (String contextWord : contextWords) {
                int contextIndex = vocabulary.getIndex(contextWord);
                updateEmbeddings(centerIndex, contextIndex);
            }
        }
    }

    private void updateEmbeddings(int centerIndex, int contextIndex) {
        // 前向传播
        double[] inputVector = inputEmbeddings[centerIndex];
        double[] outputVector = outputEmbeddings[contextIndex];

        double[] hidden = multiply(inputVector, outputVector);
        double[] softmaxOutput = softmax(hidden);

        // 计算误差
        double[] error = new double[softmaxOutput.length];
        error[contextIndex] = 1.0 - softmaxOutput[contextIndex];

        // 反向传播更新
        updateOutputWeights(error);
        updateInputWeights(error, inputVector);
    }

    public double[] getWordVector(String word) {
        int index = vocabulary.getIndex(word);
        if (index >= 0) {
            return inputEmbeddings[index].clone();
        }
        return new double[embeddingSize];
    }

    public double getSimilarity(String word1, String word2) {
        double[] vec1 = getWordVector(word1);
        double[] vec2 = getWordVector(word2);
        return cosineSimilarity(vec1, vec2);
    }

    private double cosineSimilarity(double[] vec1, double[] vec2) {
        double dotProduct = 0;
        double norm1 = 0, norm2 = 0;

        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            norm1 += vec1[i] * vec1[i];
            norm2 += vec2[i] * vec2[i];
        }

        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
    }
}
```

---

## 题目4: ⭐⭐⭐⭐ 文本分类算法与评估指标

**问题描述**:
请详细说明常见的文本分类算法，包括朴素贝叶斯、SVM、深度学习等方法的原理和适用场景，并说明准确率、精确率、召回率、F1-score等评估指标的计算和应用。

**答案要点**:
- **传统方法**: 朴素贝叶斯、SVM、逻辑回归的原理
- **深度学习方法**: CNN、RNN、Transformer在文本分类中的应用
- **评估指标**: 不同指标的定义和适用场景
- **类别不平衡**: 处理不平衡数据集的策略
- **多分类处理**: 一对多、多对多分类方法

**核心原理**:
1. 文本分类是监督学习的重要应用
2. 不同算法适用于不同的数据规模和特征表示
3. 评估指标需要根据具体业务场景选择
4. 类别不平衡需要特殊的处理策略

**核心代码示例**:
```java
// 文本分类器接口
public interface TextClassifier {
    public ClassificationResult classify(String text);
    public void train(List<TextClassificationInstance> trainingData);
}

// 朴素贝叶斯分类器
public class NaiveBayesClassifier implements TextClassifier {
    private final Map<String, Map<String, Double>> likelihoods; // P(word|class)
    private final Map<String, Double> priors; // P(class)
    private final Vocabulary vocabulary;

    @Override
    public void train(List<TextClassificationInstance> trainingData) {
        calculatePriors(trainingData);
        calculateLikelihoods(trainingData);
    }

    @Override
    public ClassificationResult classify(String text) {
        List<String> words = tokenize(text);
        Map<String, Double> posteriors = new HashMap<>();

        for (String className : priors.keySet()) {
            double logProb = Math.log(priors.get(className));

            for (String word : words) {
                double wordLikelihood = likelihoods.get(className)
                    .getOrDefault(word, 1e-10); // 拉普拉斯平滑
                logProb += Math.log(wordLikelihood);
            }

            posteriors.put(className, logProb);
        }

        String predictedClass = argmax(posteriors);
        double confidence = calculateConfidence(posteriors, predictedClass);

        return new ClassificationResult(predictedClass, confidence);
    }

    private double calculateConfidence(Map<String, Double> posteriors, String predictedClass) {
        double maxLogProb = posteriors.get(predictedClass);
        double sumExp = posteriors.values().stream()
            .mapToDouble(Math::exp)
            .sum();

        return Math.exp(maxLogProb) / sumExp;
    }
}

// 评估指标计算器
public class ClassificationMetrics {
    public static double calculateAccuracy(List<String> predicted, List<String> actual) {
        int correct = 0;
        for (int i = 0; i < predicted.size(); i++) {
            if (predicted.get(i).equals(actual.get(i))) {
                correct++;
            }
        }
        return (double) correct / predicted.size();
    }

    public static PrecisionRecall calculatePrecisionRecall(
            List<String> predicted, List<String> actual, String positiveClass) {

        int truePositive = 0;
        int falsePositive = 0;
        int falseNegative = 0;

        for (int i = 0; i < predicted.size(); i++) {
            String pred = predicted.get(i);
            String act = actual.get(i);

            if (pred.equals(positiveClass)) {
                if (act.equals(positiveClass)) {
                    truePositive++;
                } else {
                    falsePositive++;
                }
            } else if (act.equals(positiveClass)) {
                falseNegative++;
            }
        }

        double precision = truePositive + falsePositive > 0 ?
            (double) truePositive / (truePositive + falsePositive) : 0;
        double recall = truePositive + falseNegative > 0 ?
            (double) truePositive / (truePositive + falseNegative) : 0;

        return new PrecisionRecall(precision, recall);
    }

    public static double calculateF1Score(double precision, double recall) {
        return precision + recall > 0 ? 2 * precision * recall / (precision + recall) : 0;
    }
}
```

---

## 题目5: ⭐⭐⭐⭐⭐ 序列标注任务与CRF算法

**问题描述**:
请详细说明序列标注任务的特点，包括命名实体识别、词性标注等，并解释条件随机场(CRF)的原理和在序列标注中的应用。

**答案要点**:
- **序列标注**: 定义、任务类型和应用场景
- **特征工程**: 字符级、词级、句级特征的设计
- **CRF原理**: 概率图模型、特征函数和解码算法
- **解码算法**: Viterbi算法在序列预测中的应用
- **应用案例**: NER、POS、分词等实际应用

**核心原理**:
1. 序列标注需要考虑标签之间的依赖关系
2. CRF使用特征函数建模局部和全局特征
3. Viterbi算法高效地找到最优标签序列
4. 特征函数的设计直接影响模型性能

**核心代码示例**:
```java
// 条件随机场核心结构
public class ConditionalRandomField {
    private final List<String> labels;
    private final List<FeatureFunction> featureFunctions;
    private final double[] weights;

    public String[] predictSequence(String[] tokens, List<String> possibleLabels) {
        ViterbiAlgorithm viterbi = new ViterbiAlgorithm(labels, featureFunctions, weights);
        return viterbi.decode(tokens, possibleLabels);
    }

    public void train(List<SequenceTrainingInstance> trainingData) {
        GradientDescent optimizer = new GradientDescent();

        for (int epoch = 0; epoch < 100; epoch++) {
            for (SequenceTrainingInstance instance : trainingData) {
                double[] gradient = calculateGradient(instance);
                optimizer.updateWeights(weights, gradient);
            }
        }
    }

    private double[] calculateGradient(SequenceTrainingInstance instance) {
        // 计算对数似然梯度
        String[] tokens = instance.getTokens();
        String[] trueLabels = instance.getLabels();
        String[] predictedLabels = predictSequence(tokens, labels);

        return subtractGradient(trueLabels, predictedLabels, tokens);
    }
}

// Viterbi算法实现
public class ViterbiAlgorithm {
    private final List<String> labels;
    private final List<FeatureFunction> featureFunctions;
    private final double[] weights;

    public String[] decode(String[] tokens, List<String> possibleLabels) {
        int sequenceLength = tokens.length;
        int numLabels = possibleLabels.size();

        // DP表: [position][label]
        double[][] viterbi = new double[sequenceLength][numLabels];
        int[][] backpointer = new int[sequenceLength][numLabels];

        // 初始化
        for (int j = 0; j < numLabels; j++) {
            viterbi[0][j] = calculateInitialScore(tokens[0], possibleLabels.get(j), weights);
            backpointer[0][j] = -1;
        }

        // 递推
        for (int t = 1; t < sequenceLength; t++) {
            for (int j = 0; j < numLabels; j++) {
                double maxScore = Double.NEGATIVE_INFINITY;
                int bestPrevLabel = -1;

                for (int k = 0; k < numLabels; k++) {
                    double transitionScore = calculateTransitionScore(
                        possibleLabels.get(k), possibleLabels.get(j), weights);
                    double emissionScore = calculateEmissionScore(
                        tokens[t], possibleLabels.get(j), weights);

                    double score = viterbi[t-1][k] + transitionScore + emissionScore;

                    if (score > maxScore) {
                        maxScore = score;
                        bestPrevLabel = k;
                    }
                }

                viterbi[t][j] = maxScore;
                backpointer[t][j] = bestPrevLabel;
            }
        }

        // 回溯最优路径
        return findBestPath(viterbi, backpointer, possibleLabels);
    }

    private String[] findBestPath(double[][] viterbi, int[][] backpointer, List<String> labels) {
        int sequenceLength = viterbi.length;
        int numLabels = labels.size();
        String[] bestPath = new String[sequenceLength];

        // 找到最后一个位置的最佳标签
        int bestLastLabel = 0;
        double maxScore = viterbi[sequenceLength-1][0];

        for (int j = 1; j < numLabels; j++) {
            if (viterbi[sequenceLength-1][j] > maxScore) {
                maxScore = viterbi[sequenceLength-1][j];
                bestLastLabel = j;
            }
        }

        bestPath[sequenceLength-1] = labels.get(bestLastLabel);

        // 回溯
        for (int t = sequenceLength-2; t >= 0; t--) {
            bestLastLabel = backpointer[t+1][bestLastLabel];
            bestPath[t] = labels.get(bestLastLabel);
        }

        return bestPath;
    }
}

// 特征函数接口
public interface FeatureFunction {
    double evaluate(String[] tokens, int position, String currentLabel, String previousLabel);
    int getFeatureIndex();
}

public class WordFeature implements FeatureFunction {
    private final String word;
    private final String label;
    private final int featureIndex;

    public WordFeature(String word, String label, int featureIndex) {
        this.word = word;
        this.label = label;
        this.featureIndex = featureIndex;
    }

    public double evaluate(String[] tokens, int position, String currentLabel, String previousLabel) {
        return currentLabel.equals(label) && tokens[position].equals(word) ? 1.0 : 0.0;
    }

    public int getFeatureIndex() {
        return featureIndex;
    }
}
```

---

**总结**: 文本预处理与特征工程是NLP任务的基础环节，决定了后续模型的上限。从基础的文本清洗到高级的特征提取，每一步都需要结合具体任务特点进行优化。理解这些技术原理对于构建高效的NLP系统至关重要。