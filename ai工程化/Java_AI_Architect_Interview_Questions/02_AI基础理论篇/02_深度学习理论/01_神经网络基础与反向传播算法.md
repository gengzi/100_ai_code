# 神经网络基础与反向传播算法

## 题目1: ⭐⭐⭐ 感知机与多层感知器的核心原理

**问题描述**:
请详细解释感知机(Perceptron)和多层感知器(MLP)的工作原理，说明它们的区别和联系，并分析为什么多层感知器能够解决非线性问题。

**答案要点**:
- **感知机原理**: 单层线性分类器，权重更新规则
- **激活函数**: 阶跃函数、Sigmoid、ReLU等的作用和特点
- **多层感知机**: 多层结构、非线性映射能力
- **反向传播**: 梯度计算和权重更新机制

**核心原理**:
1. 感知机是最简单的神经网络模型，只能解决线性可分问题
2. 多层感知器通过隐藏层实现非线性特征变换
3. 反向传播算法使用链式法则计算梯度
4. 激活函数引入非线性，增强模型表达能力

**简洁示例代码**:
```java
// 感知机核心逻辑
public class Perceptron {
    private double[] weights;
    private double bias;
    private double learningRate = 0.1;

    public int predict(double[] inputs) {
        double sum = bias;
        for (int i = 0; i < inputs.length; i++) {
            sum += weights[i] * inputs[i];
        }
        return sum > 0 ? 1 : -1;
    }

    public void train(double[] inputs, int target) {
        int prediction = predict(inputs);
        int error = target - prediction;

        if (error != 0) {
            for (int i = 0; i < weights.length; i++) {
                weights[i] += learningRate * error * inputs[i];
            }
            bias += learningRate * error;
        }
    }
}

// 多层感知器神经元
public class Neuron {
    private double[] weights;
    private double bias;
    private ActivationFunction activation;

    public double activate(double[] inputs) {
        double sum = bias;
        for (int i = 0; i < inputs.length; i++) {
            sum += weights[i] * inputs[i];
        }
        return activation.apply(sum);
    }
}

// 常用激活函数
public interface ActivationFunction {
    double apply(double x);
}

class ReLU implements ActivationFunction {
    public double apply(double x) { return Math.max(0, x); }
}

class Sigmoid implements ActivationFunction {
    public double apply(double x) { return 1 / (1 + Math.exp(-x)); }
}
```

---

## 题目2: ⭐⭐⭐⭐ 反向传播算法的数学原理和实现

**问题描述**:
请详细解释反向传播算法的数学原理，包括梯度计算、链式法则的应用，以及如何处理梯度消失和梯度爆炸问题。

**答案要点**:
- **损失函数**: MSE、交叉熵等的选择和特性
- **链式法则**: 多层网络的梯度计算方法
- **梯度问题**: 梯度消失/爆炸的原因和解决方案
- **优化技术**: 学习率调度、正则化、批归一化

**核心原理**:
1. 反向传播使用链式法则高效计算梯度
2. 损失函数决定了梯度的计算方式
3. 深层网络容易出现梯度消失问题
4. 合理的初始化和归一化可以缓解梯度问题

**简洁示例代码**:
```java
// 反向传播核心算法
public class BackPropagation {
    private List<Layer> layers;
    private double learningRate;

    public void train(double[] inputs, double[] targets) {
        // 前向传播
        double[] outputs = forwardPropagation(inputs);

        // 计算输出层误差
        double[] errors = new double[outputs.length];
        for (int i = 0; i < outputs.length; i++) {
            errors[i] = targets[i] - outputs[i];
        }

        // 反向传播误差
        backwardPropagation(errors);

        // 更新权重
        updateWeights();
    }

    private void backwardPropagation(double[] errors) {
        double[][] layerErrors = new double[layers.size()][];
        layerErrors[layerErrors.length - 1] = errors;

        // 从输出层向输入层反向计算
        for (int l = layers.size() - 2; l >= 0; l--) {
            Layer currentLayer = layers.get(l);
            Layer nextLayer = layers.get(l + 1);

            double[] currentErrors = new double[currentLayer.getOutputSize()];

            for (int i = 0; i < currentErrors.length; i++) {
                double sum = 0;
                for (int j = 0; j < nextLayer.getOutputSize(); j++) {
                    sum += layerErrors[l + 1][j] * nextLayer.getWeight(j, i);
                }
                currentErrors[i] = sum * currentLayer.getActivationDerivative(i);
            }

            layerErrors[l] = currentErrors;
        }
    }
}

// 梯度裁剪防止梯度爆炸
public class GradientClipping {
    public static void clipGradients(double[][] gradients, double threshold) {
        double norm = 0;
        for (double[] layer : gradients) {
            for (double grad : layer) {
                norm += grad * grad;
            }
        }
        norm = Math.sqrt(norm);

        if (norm > threshold) {
            double scale = threshold / norm;
            for (double[] layer : gradients) {
                for (int i = 0; i < layer.length; i++) {
                    layer[i] *= scale;
                }
            }
        }
    }
}
```

---

## 题目3: ⭐⭐⭐⭐ CNN卷积神经网络的设计原理

**问题描述**:
请详细解释CNN的核心概念，包括卷积操作、池化层、感受野、特征图等，并分析CNN在图像识别任务中的优势。

**答案要点**:
- **卷积操作**: 局部连接、权重共享、平移不变性
- **池化层**: 下采样、最大池化、平均池化
- **网络架构**: LeNet、AlexNet、VGG、ResNet的演进
- **参数共享**: 大幅减少参数量的原理

**核心原理**:
1. 卷积操作提取局部特征并保持空间关系
2. 池化层降低维度并增强平移不变性
3. 深层网络学习层次化特征表示
4. 参数共享大幅减少模型参数数量

**简洁示例代码**:
```java
// 卷积层实现
public class ConvLayer {
    private int[][][] kernels; // 卷积核
    private int kernelSize;
    private int stride;
    private int padding;

    public double[][][] convolve(double[][][] input) {
        int outputDepth = kernels.length;
        int outputHeight = (input[0].length + 2 * padding - kernelSize) / stride + 1;
        int outputWidth = (input[0][0].length + 2 * padding - kernelSize) / stride + 1;

        double[][][] output = new double[outputDepth][outputHeight][outputWidth];

        for (int d = 0; d < outputDepth; d++) {
            for (int h = 0; h < outputHeight; h++) {
                for (int w = 0; w < outputWidth; w++) {
                    output[d][h][w] = singleConvolution(input, kernels[d], h, w);
                }
            }
        }

        return output;
    }

    private double singleConvolution(double[][][] input, int[][] kernel, int startH, int startW) {
        double sum = 0;
        for (int i = 0; i < kernel.length; i++) {
            for (int j = 0; j < kernel[0].length; j++) {
                int h = startH + i;
                int w = startW + j;
                if (isValidPosition(input, h, w)) {
                    sum += input[0][h][w] * kernel[i][j];
                }
            }
        }
        return sum;
    }
}

// 最大池化层
public class MaxPoolLayer {
    private int poolSize;
    private int stride;

    public double[][][] maxPool(double[][][] input) {
        int outputHeight = input[0].length / stride;
        int outputWidth = input[0][0].length / stride;

        double[][][] output = new double[input.length][outputHeight][outputWidth];

        for (int d = 0; d < input.length; d++) {
            for (int h = 0; h < outputHeight; h++) {
                for (int w = 0; w < outputWidth; w++) {
                    output[d][h][w] = findMax(input[d], h * stride, w * stride);
                }
            }
        }

        return output;
    }

    private double findMax(double[][] input, int startH, int startW) {
        double max = Double.NEGATIVE_INFINITY;
        for (int i = 0; i < poolSize; i++) {
            for (int j = 0; j < poolSize; j++) {
                int h = startH + i;
                int w = startW + j;
                if (isValidPosition(input, h, w)) {
                    max = Math.max(max, input[h][w]);
                }
            }
        }
        return max;
    }
}
```

---

## 题目4: ⭐⭐⭐⭐⭐ RNN循环神经网络与序列建模

**问题描述**:
请详细解释RNN的结构和工作原理，说明其如何处理序列数据，并分析梯度消失问题及LSTM、GRU等改进方案的原理。

**答案要点**:
- **循环结构**: 隐藏状态的传递和更新机制
- **序列处理**: 变长序列的处理方法
- **梯度消失**: 长序列训练中的梯度衰减问题
- **门控机制**: LSTM和GRU的门控结构和信息流控制

**核心原理**:
1. RNN通过循环连接处理序列数据，保持历史信息
2. 隐藏状态在每个时间步都会更新并传递到下一个时间步
3. 长序列训练容易出现梯度消失问题
4. LSTM和GRU通过门控机制控制信息流动

**简洁示例代码**:
```java
// 基础RNN单元
public class RNNCell {
    private double[] inputWeights;
    private double[] hiddenWeights;
    private double hiddenBias;
    private ActivationFunction activation;

    public double[] forward(double[] input, double[] previousHidden) {
        double sum = hiddenBias;

        // 输入加权
        for (int i = 0; i < input.length; i++) {
            sum += input[i] * inputWeights[i];
        }

        // 隐藏状态加权
        for (int i = 0; i < previousHidden.length; i++) {
            sum += previousHidden[i] * hiddenWeights[i];
        }

        double[] newHidden = new double[previousHidden.length];
        Arrays.fill(newHidden, activation.apply(sum));

        return newHidden;
    }
}

// LSTM单元
public class LSTMCell {
    private double[][] inputGateWeights;
    private double[][] forgetGateWeights;
    private double[][] outputGateWeights;
    private double[][] candidateWeights;

    public double[] forward(double[] input, double[] previousHidden, double[] previousCell) {
        // 输入门
        double[] inputGate = sigmoid(matrixMultiply(inputGateWeights, input) +
                                   matrixMultiply(previousHidden, previousHidden));

        // 遗忘门
        double[] forgetGate = sigmoid(matrixMultiply(forgetGateWeights, input) +
                                    matrixMultiply(previousHidden, previousHidden));

        // 候选值
        double[] candidate = tanh(matrixMultiply(candidateWeights, input) +
                               matrixMultiply(previousHidden, previousHidden));

        // 输出门
        double[] outputGate = sigmoid(matrixMultiply(outputGateWeights, input) +
                                    matrixMultiply(previousHidden, previousHidden));

        // 新细胞状态
        double[] newCell = new double[previousCell.length];
        for (int i = 0; i < newCell.length; i++) {
            newCell[i] = forgetGate[i] * previousCell[i] + inputGate[i] * candidate[i];
        }

        // 新隐藏状态
        double[] newHidden = new double[previousHidden.length];
        for (int i = 0; i < newHidden.length; i++) {
            newHidden[i] = outputGate[i] * tanh(newCell[i]);
        }

        return newHidden;
    }

    private double[] sigmoid(double[] x) {
        double[] result = new double[x.length];
        for (int i = 0; i < x.length; i++) {
            result[i] = 1 / (1 + Math.exp(-x[i]));
        }
        return result;
    }

    private double[] tanh(double[] x) {
        double[] result = new double[x.length];
        for (int i = 0; i < x.length; i++) {
            double expX = Math.exp(x[i]);
            double expMinusX = Math.exp(-x[i]);
            result[i] = (expX - expMinusX) / (expX + expMinusX);
        }
        return result;
    }
}
```

---

## 题目5: ⭐⭐⭐⭐⭐ Transformer架构与自注意力机制

**问题描述**:
请详细解释Transformer的核心创新点，包括自注意力机制、位置编码、编码器-解码器架构，并分析其相比RNN和CNN的优势。

**答案要点**:
- **自注意力机制**: Query、Key、Value的计算和注意力权重
- **多头注意力**: 多个子空间的并行注意力计算
- **位置编码**: 解决序列位置信息的编码方式
- **架构优势**: 并行计算、长距离依赖、无循环结构

**核心原理**:
1. 自注意力机制通过Query-Key-Value计算序列内部依赖关系
2. 多头注意力能够从不同表示空间捕获多种依赖关系
3. 位置编码提供序列位置信息
4. Transformer实现了完全并行化的序列建模

**简洁示例代码**:
```java
// 自注意力机制核心实现
public class SelfAttention {
    private int embeddingSize;
    private int numHeads;

    public double[][] forward(double[][] input) {
        // 输入形状: [seqLength, embeddingSize]

        // 1. 线性变换生成Q、K、V
        double[][] queries = linearProjection(input, "query");
        double[][] keys = linearProjection(input, "key");
        double[][] values = linearProjection(input, "value");

        // 2. 计算注意力分数
        double[][] attentionScores = new double[input.length][input.length];
        for (int i = 0; i < input.length; i++) {
            for (int j = 0; j < input.length; j++) {
                // 点积注意力
                attentionScores[i][j] = dotProduct(queries[i], keys[j]) /
                                         Math.sqrt(embeddingSize);
            }
        }

        // 3. 应用softmax得到注意力权重
        double[][] attentionWeights = softmax(attentionScores);

        // 4. 加权求和得到输出
        double[][] output = new double[input.length][embeddingSize];
        for (int i = 0; i < input.length; i++) {
            for (int j = 0; j < input.length; j++) {
                double weight = attentionWeights[i][j];
                for (int k = 0; k < embeddingSize; k++) {
                    output[i][k] += weight * values[j][k];
                }
            }
        }

        return output;
    }

    private double[][] softmax(double[][] scores) {
        double[][] result = new double[scores.length][scores[0].length];

        for (int i = 0; i < scores.length; i++) {
            // 减去最大值防止数值溢出
            double maxScore = Arrays.stream(scores[i]).max().orElse(0);
            double sumExp = 0;

            for (int j = 0; j < scores[i].length; j++) {
                double exp = Math.exp(scores[i][j] - maxScore);
                result[i][j] = exp;
                sumExp += exp;
            }

            for (int j = 0; j < scores[i].length; j++) {
                result[i][j] /= sumExp;
            }
        }

        return result;
    }
}

// 位置编码实现
public class PositionalEncoding {
    public double[][] encode(int seqLength, int embeddingSize) {
        double[][] encoding = new double[seqLength][embeddingSize];

        for (int pos = 0; pos < seqLength; pos++) {
            for (int i = 0; i < embeddingSize; i++) {
                if (i % 2 == 0) {
                    // 偶数位置使用sin
                    int halfDim = i / 2;
                    encoding[pos][i] = Math.sin(pos / Math.pow(10000, 2.0 * halfDim / embeddingSize));
                } else {
                    // 奇数位置使用cos
                    int halfDim = (i - 1) / 2;
                    encoding[pos][i] = Math.cos(pos / Math.pow(10000, 2.0 * halfDim / embeddingSize));
                }
            }
        }

        return encoding;
    }
}

// 多头注意力
public class MultiHeadAttention {
    private List<SelfAttention> heads;
    private int numHeads;
    private int headSize;

    public double[][] forward(double[][] input) {
        List<double[][]> headOutputs = new ArrayList<>();

        // 并行计算多个注意力头
        for (SelfAttention head : heads) {
            headOutputs.add(head.forward(input));
        }

        // 拼接所有头的输出
        return concatenateHeads(headOutputs);
    }

    private double[][] concatenateHeads(List<double[][]> headOutputs) {
        int seqLength = headOutputs.get(0).length;
        int embeddingSize = headOutputs.get(0)[0].length * numHeads;

        double[][] result = new double[seqLength][embeddingSize];

        for (int i = 0; i < seqLength; i++) {
            int col = 0;
            for (double[][] headOutput : headOutputs) {
                for (int j = 0; j < headOutput[0].length; j++) {
                    result[i][col++] = headOutput[i][j];
                }
            }
        }

        return result;
    }
}
```

---

## 题目6: ⭐⭐⭐ 优化算法在深度学习中的应用

**问题描述**:
请详细说明梯度下降、Adam、学习率调度等优化算法的原理，并分析它们在深度学习训练中的作用和选择策略。

**答案要点**:
- **梯度下降**: 批量梯度下降、随机梯度下降、小批量梯度下降
- **自适应优化**: Adam、RMSprop、AdaGrad等自适应学习率算法
- **学习率调度**: 学习率衰减、热重启、余弦退火等调度策略
- **正则化技术**: L1/L2正则化、Dropout、Batch Normalization

**核心原理**:
1. 梯度下降通过负梯度方向更新参数来最小化损失函数
2. 自适应算法根据历史梯度信息调整学习率
3. 学习率调度控制训练过程中的学习率变化
4. 正则化技术防止过拟合并提高泛化能力

**简洁示例代码**:
```java
// 优化器接口
public interface Optimizer {
    void updateParameters(double[][] parameters, double[][] gradients);
    void reset();
}

// Adam优化器
public class AdamOptimizer implements Optimizer {
    private double learningRate;
    private double beta1;
    private double beta2;
    private double epsilon;

    private double[][] m; // 一阶矩估计
    private double[][] v; // 二阶矩估计
    private int timestep;

    public AdamOptimizer(double learningRate) {
        this.learningRate = learningRate;
        this.beta1 = 0.9;
        this.beta2 = 0.999;
        this.epsilon = 1e-8;
        this.timestep = 0;
    }

    public void updateParameters(double[][] parameters, double[][] gradients) {
        timestep++;

        // 初始化矩估计
        if (m == null) {
            m = new double[gradients.length][gradients[0].length];
            v = new double[gradients.length][gradients[0].length];
        }

        // 更新一阶矩估计
        for (int i = 0; i < gradients.length; i++) {
            for (int j = 0; j < gradients[0].length; j++) {
                m[i][j] = beta1 * m[i][j] + (1 - beta1) * gradients[i][j];
            }
        }

        // 更新二阶矩估计
        for (int i = 0; i < gradients.length; i++) {
            for (int j = 0; j < gradients[0].length; j++) {
                v[i][j] = beta2 * v[i][j] + (1 - beta2) * gradients[i][j] * gradients[i][j];
            }
        }

        // 计算偏差修正后的估计
        double[][] mHat = new double[m.length][m[0].length];
        double[][] vHat = new double[v.length][v[0].length];

        for (int i = 0; i < m.length; i++) {
            for (int j = 0; j < m[0].length; j++) {
                mHat[i][j] = m[i][j] / (1 - Math.pow(beta1, timestep));
                vHat[i][j] = v[i][j] / (1 - Math.pow(beta2, timestep));
            }
        }

        // 更新参数
        for (int i = 0; i < parameters.length; i++) {
            for (int j = 0; j < parameters[0].length; j++) {
                parameters[i][j] -= learningRate * mHat[i][j] /
                    (Math.sqrt(vHat[i][j]) + epsilon);
            }
        }
    }
}

// 学习率调度器
public class LearningRateScheduler {
    public static double cosineAnnealing(int epoch, int maxEpochs, double initialLR) {
        return initialLR * (0.5 * (1 + Math.cos(Math.PI * epoch / maxEpochs)));
    }

    public static double stepDecay(int epoch, int decaySteps, double decayRate, double initialLR) {
        return initialLR * Math.pow(decayRate, Math.floor(epoch / decaySteps));
    }

    public static double warmup(int epoch, int warmupEpochs, double maxLR) {
        if (epoch < warmupEpochs) {
            return maxLR * epoch / warmupEpochs;
        }
        return maxLR;
    }
}

// 梯度裁剪
public class GradientClipping {
    public static void clipByNorm(double[][] gradients, double maxNorm) {
        double totalNorm = 0;

        for (double[] layer : gradients) {
            for (double grad : layer) {
                totalNorm += grad * grad;
            }
        }

        totalNorm = Math.sqrt(totalNorm);

        if (totalNorm > maxNorm) {
            double scale = maxNorm / totalNorm;
            for (double[] layer : gradients) {
                for (int i = 0; i < layer.length; i++) {
                    layer[i] *= scale;
                }
            }
        }
    }
}
```

---

## 题目7: ⭐⭐⭐⭐ 正则化技术与过拟合预防

**问题描述**:
请详细说明深度学习中的正则化技术，包括L1/L2正则化、Dropout、Batch Normalization等，并分析它们在防止过拟合中的作用机制。

**答案要点**:
- **权重正则化**: L1/L2正则化对权重的约束作用
- **Dropout**: 随机失活神经元防止协同适应
- **Batch Normalization**: 归一化激活值分布，加速训练
- **数据增强**: 增加训练数据多样性
- **早停策略**: 基于验证集性能的提前停止

**核心原理**:
1. 正则化通过添加约束项限制模型复杂度
2. Dropout通过随机失活神经元增强模型泛化能力
3. Batch Normalization稳定训练过程并允许更高学习率
4. 多种正则化技术通常结合使用获得最佳效果

**简洁示例代码**:
```java
// L2正则化
public class L2Regularization {
    private final double lambda;

    public L2Regularization(double lambda) {
        this.lambda = lambda;
    }

    public double calculateRegularization(double[][] weights) {
        double sum = 0;
        for (double[] layer : weights) {
            for (double weight : layer) {
                sum += weight * weight;
            }
        }
        return 0.5 * lambda * sum;
    }

    public void updateGradients(double[][] gradients, double[][] weights) {
        for (int i = 0; i < weights.length; i++) {
            for (int j = 0; j < weights[0].length; j++) {
                gradients[i][j] += lambda * weights[i][j];
            }
        }
    }
}

// Dropout层
public class DropoutLayer {
    private double dropoutRate;
    private boolean training;
    private double[][] mask;

    public double[][] forward(double[][] input, boolean training) {
        this.training = training;

        if (!training) {
            return input; // 测试时不应用dropout
        }

        return applyDropout(input);
    }

    private double[][] applyDropout(double[][] input) {
        double[][] output = new double[input.length][input[0].length];
        mask = new double[input.length][input[0].length];

        Random random = new Random();
        double scale = 1.0 / (1.0 - dropoutRate);

        for (int i = 0; i < input.length; i++) {
            for (int j = 0; j < input[0].length; j++) {
                if (random.nextDouble() > dropoutRate) {
                    output[i][j] = input[i][j] * scale;
                    mask[i][j] = scale;
                } else {
                    output[i][j] = 0;
                    mask[i][j] = 0;
                }
            }
        }

        return output;
    }

    public double[][] backward(double[][] gradients) {
        if (!training || mask == null) {
            return gradients;
        }

        double[][] output = new double[gradients.length][gradients[0].length];

        for (int i = 0; i < gradients.length; i++) {
            for (int j = 0; j < gradients[0].length; j++) {
                output[i][j] = gradients[i][j] * mask[i][j];
            }
        }

        return output;
    }
}

// Batch Normalization
public class BatchNormalization {
    private double[][] gamma; // 缩放参数
    private double[][] beta;  // 偏移参数
    private double[][] runningMean;
    private double[][] runningVar;
    private double epsilon = 1e-8;
    private double momentum = 0.99;

    public double[][] forward(double[][] input, boolean training) {
        int batchSize = input.length;
        int featureSize = input[0].length;

        if (training) {
            // 计算均值和方差
            double[] mean = new double[featureSize];
            double[] variance = new double[featureSize];

            for (int j = 0; j < featureSize; j++) {
                for (int i = 0; i < batchSize; i++) {
                    mean[j] += input[i][j];
                }
                mean[j] /= batchSize;

                for (int i = 0; i < batchSize; i++) {
                    double diff = input[i][j] - mean[j];
                    variance[j] += diff * diff;
                }
                variance[j] /= batchSize;
            }

            // 标准化并应用缩放和偏移
            double[][] output = new double[batchSize][featureSize];
            for (int i = 0; i < batchSize; i++) {
                for (int j = 0; j < featureSize; j++) {
                    double normalized = (input[i][j] - mean[j]) /
                                      Math.sqrt(variance[j] + epsilon);
                    output[i][j] = gamma[0][j] * normalized + beta[0][j];
                }
            }

            // 更新运行时统计
            updateRunningMean(mean);
            updateRunningVar(variance);

            return output;
        } else {
            // 测试时使用运行时统计
            double[][] output = new double[batchSize][featureSize];
            for (int i = 0; i < batchSize; i++) {
                for (int j = 0; j < featureSize; j++) {
                    double normalized = (input[i][j] - runningMean[0][j]) /
                                      Math.sqrt(runningVar[0][j] + epsilon);
                    output[i][j] = gamma[0][j] * normalized + beta[0][j];
                }
            }
            return output;
        }
    }

    private void updateRunningMean(double[] batchMean) {
        for (int j = 0; j < runningMean[0].length; j++) {
            runningMean[0][j] = momentum * runningMean[0][j] + (1 - momentum) * batchMean[j];
        }
    }

    private void updateRunningVar(double[] batchVar) {
        for (int j = 0; j < runningVar[0].length; j++) {
            runningVar[0][j] = momentum * runningVar[0][j] + (1 - momentum) * batchVar[j];
        }
    }
}

// 早停策略
public class EarlyStopping {
    private int patience = 10;
    private double minDelta = 1e-4;
    private double bestScore = Double.NEGATIVE_INFINITY;
    private int waitCount = 0;

    public boolean shouldStop(double currentScore) {
        if (currentScore > bestScore + minDelta) {
            bestScore = currentScore;
            waitCount = 0;
            return false;
        } else {
            waitCount++;
            return waitCount >= patience;
        }
    }
}
```

---

**总结**: 深度学习的核心在于通过多层非线性变换学习数据特征表示。从基础的感知机到复杂的Transformer架构，每个组件都有其独特的作用和适用场景。理解这些核心原理对于设计有效的AI系统至关重要。</think>