# 线性代数在AI中的应用

## 题目1: ⭐ 矩阵运算在神经网络中的基础应用

**问题描述**:
在深度学习中，前向传播和反向传播都大量使用矩阵运算。请解释矩阵乘法在神经网络中的作用，并用Java实现一个简单的前向传播计算。

**答案要点**:
- **前向传播**: 输入向量与权重矩阵相加偏置
- **维度匹配**: 确保矩阵运算的维度正确
- **批处理**: 支持多个样本的批量计算
- **内存效率**: 利用矩阵运算的并行性

**代码示例**:
```java
public class NeuralNetworkForwardProp {
    public static double[] forwardPropagation(double[] input,
                                            double[][] weights,
                                            double[] bias) {
        // 验证维度
        if (input.length != weights[0].length) {
            throw new IllegalArgumentException("输入维度与权重矩阵不匹配");
        }

        double[] output = new double[weights.length];

        // 矩阵乘法: output = input * weights^T + bias
        for (int i = 0; i < weights.length; i++) {
            double sum = bias[i];
            for (int j = 0; j < input.length; j++) {
                sum += input[j] * weights[i][j];
            }
            output[i] = sum;
        }

        return output;
    }

    // 批量前向传播
    public static double[][] batchForwardPropagation(double[][] inputs,
                                                   double[][] weights,
                                                   double[] bias) {
        double[][] outputs = new double[inputs.length][weights.length];

        for (int batch = 0; batch < inputs.length; batch++) {
            outputs[batch] = forwardPropagation(inputs[batch], weights, bias);
        }

        return outputs;
    }
}
```

---

## 题目2: ⭐⭐ 特征值分解在主成分分析(PCA)中的应用

**问题描述**:
主成分分析是降维的重要技术，其核心是特征值分解。请说明特征值分解在PCA中的作用，并用Java实现一个PCA算法。

**答案要点**:
- **协方差矩阵**: 计算数据的协方差矩阵
- **特征值分解**: 求解特征值和特征向量
- **主成分选择**: 选择前k个最大的特征值对应的主成分
- **数据投影**: 将原始数据投影到主成分空间

**代码示例**:
```java
public class PCA {
    private double[][] components;  // 主成分
    private double[] explainedVariance;  // 解释方差
    private double[] mean;  // 均值

    public void fit(double[][] X) {
        int n_samples = X.length;
        int n_features = X[0].length;

        // 1. 计算均值
        this.mean = calculateMean(X);

        // 2. 中心化数据
        double[][] centered = centerData(X, mean);

        // 3. 计算协方差矩阵
        double[][] covarianceMatrix = calculateCovarianceMatrix(centered);

        // 4. 特征值分解
        EigenDecomposition decomposition = eigenDecomposition(covarianceMatrix);
        this.components = decomposition.getEigenvectors();
        this.explainedVariance = decomposition.getEigenvalues();
    }

    public double[][] transform(double[][] X) {
        double[][] centered = centerData(X, mean);
        return matrixMultiply(centered, transpose(components));
    }

    private double[] calculateMean(double[][] X) {
        int n_features = X[0].length;
        double[] mean = new double[n_features];

        for (double[] sample : X) {
            for (int j = 0; j < n_features; j++) {
                mean[j] += sample[j];
            }
        }

        for (int j = 0; j < n_features; j++) {
            mean[j] /= X.length;
        }

        return mean;
    }

    private double[][] centerData(double[][] X, double[] mean) {
        double[][] centered = new double[X.length][X[0].length];

        for (int i = 0; i < X.length; i++) {
            for (int j = 0; j < X[0].length; j++) {
                centered[i][j] = X[i][j] - mean[j];
            }
        }

        return centered;
    }

    private double[][] calculateCovarianceMatrix(double[][] X) {
        int n_features = X[0].length;
        double[][] covariance = new double[n_features][n_features];

        for (int i = 0; i < n_features; i++) {
            for (int j = 0; j < n_features; j++) {
                double sum = 0.0;
                for (int k = 0; k < X.length; k++) {
                    sum += X[k][i] * X[k][j];
                }
                covariance[i][j] = sum / (X.length - 1);
            }
        }

        return covariance;
    }

    private EigenDecomposition eigenDecomposition(double[][] matrix) {
        // 简化的特征值分解实现
        // 实际应用中应使用专业的数值计算库如Apache Commons Math
        return new EigenDecomposition(matrix);
    }

    // 矩阵转置
    private double[][] transpose(double[][] matrix) {
        double[][] result = new double[matrix[0].length][matrix.length];
        for (int i = 0; i < matrix.length; i++) {
            for (int j = 0; j < matrix[0].length; j++) {
                result[j][i] = matrix[i][j];
            }
        }
        return result;
    }

    // 矩阵乘法
    private double[][] matrixMultiply(double[][] A, double[][] B) {
        double[][] result = new double[A.length][B[0].length];
        for (int i = 0; i < A.length; i++) {
            for (int j = 0; j < B[0].length; j++) {
                for (int k = 0; k < B.length; k++) {
                    result[i][j] += A[i][k] * B[k][j];
                }
            }
        }
        return result;
    }
}

// 特征值分解结果类
class EigenDecomposition {
    private final double[][] eigenvectors;
    private final double[] eigenvalues;

    public EigenDecomposition(double[][] matrix) {
        // 简化实现：假设已经计算好了特征值和特征向量
        int n = matrix.length;
        this.eigenvalues = new double[n];
        this.eigenvectors = new double[n][n];

        // 实际应用中这里应该实现真实的特征值分解算法
        // 如幂迭代法、QR分解等
    }

    public double[][] getEigenvectors() { return eigenvectors; }
    public double[] getEigenvalues() { return eigenvalues; }
}
```

---

## 题目3: ⭐⭐⭐ 奇异值分解(SVD)在推荐系统中的应用

**问题描述**:
奇异值分解在推荐系统中的矩阵补全技术中发挥重要作用。请解释SVD在协同过滤中的原理，并用Java实现一个基于SVD的推荐算法。

**答案要点**:
- **用户-物品矩阵**: 构建评分矩阵
- **SVD分解**: U * Σ * V^T分解
- **降维**: 保留主要奇异值
- **预测评分**: 重构矩阵预测缺失评分

**代码示例**:
```java
public class SVDRecommender {
    private double[][] userFactors;    // 用户因子矩阵 (U)
    private double[][] itemFactors;    // 物品因子矩阵 (V)
    private double[] singularValues;   // 奇异值 (Σ)
    private int numFactors;            // 降维后的因子数量

    public void train(double[][] ratingMatrix, int numFactors) {
        this.numFactors = numFactors;

        // 1. 执行SVD分解
        SVDResult svd = performSVD(ratingMatrix);

        // 2. 提取主要的奇异值和对应的向量
        this.singularValues = Arrays.copyOf(svd.singularValues, numFactors);
        this.userFactors = extractMatrixColumns(svd.leftVectors, numFactors);
        this.itemFactors = extractMatrixColumns(svd.rightVectors, numFactors);

        // 3. 将奇异值融入到因子矩阵中
        incorporateSingularValues();
    }

    public double predictRating(int userId, int itemId) {
        if (userId < 0 || userId >= userFactors.length ||
            itemId < 0 || itemId >= itemFactors.length) {
            throw new IllegalArgumentException("无效的用户或物品ID");
        }

        double rating = 0.0;
        for (int k = 0; k < numFactors; k++) {
            rating += userFactors[userId][k] * itemFactors[itemId][k];
        }

        return rating;
    }

    public List<Integer> recommendItems(int userId, int numRecommendations,
                                       Set<Integer> excludeItems) {
        List<ItemScore> itemScores = new ArrayList<>();

        for (int itemId = 0; itemId < itemFactors.length; itemId++) {
            if (!excludeItems.contains(itemId)) {
                double score = predictRating(userId, itemId);
                itemScores.add(new ItemScore(itemId, score));
            }
        }

        // 按评分排序并返回前N个推荐
        itemScores.sort((a, b) -> Double.compare(b.score, a.score));

        List<Integer> recommendations = new ArrayList<>();
        for (int i = 0; i < Math.min(numRecommendations, itemScores.size()); i++) {
            recommendations.add(itemScores.get(i).itemId);
        }

        return recommendations;
    }

    private SVDResult performSVD(double[][] matrix) {
        // 简化的SVD实现
        // 实际应用中应使用专业的数值计算库
        int m = matrix.length;
        int n = matrix[0].length;
        int rank = Math.min(m, n);

        double[][] U = new double[m][rank];
        double[][] V = new double[n][rank];
        double[] sigma = new double[rank];

        // 这里应该是真实的SVD算法实现
        // 如幂迭代法、Golub-Kahan算法等

        return new SVDResult(U, V, sigma);
    }

    private double[][] extractMatrixColumns(double[][] matrix, int numColumns) {
        double[][] result = new double[matrix.length][numColumns];
        for (int i = 0; i < matrix.length; i++) {
            System.arraycopy(matrix[i], 0, result[i], 0, numColumns);
        }
        return result;
    }

    private void incorporateSingularValues() {
        // 将奇异值的平方根分配到用户和物品因子中
        for (int k = 0; k < numFactors; k++) {
            double sqrtSigma = Math.sqrt(singularValues[k]);

            for (int i = 0; i < userFactors.length; i++) {
                userFactors[i][k] *= sqrtSigma;
            }

            for (int j = 0; j < itemFactors.length; j++) {
                itemFactors[j][k] *= sqrtSigma;
            }
        }
    }

    // SVD分解结果
    private static class SVDResult {
        final double[][] leftVectors;   // U矩阵
        final double[][] rightVectors;  // V矩阵
        final double[] singularValues;  // 奇异值

        SVDResult(double[][] leftVectors, double[][] rightVectors, double[] singularValues) {
            this.leftVectors = leftVectors;
            this.rightVectors = rightVectors;
            this.singularValues = singularValues;
        }
    }

    // 物品评分辅助类
    private static class ItemScore {
        final int itemId;
        final double score;

        ItemScore(int itemId, double score) {
            this.itemId = itemId;
            this.score = score;
        }
    }
}
```

---

## 题目4: ⭐⭐⭐⭐ 梯度下降中的线性代数优化

**问题描述**:
在大型神经网络训练中，梯度下降的计算效率至关重要。请分析线性代数运算在梯度下降中的优化作用，并实现一个支持向量化计算的优化版本。

**答案要点**:
- **向量化计算**: 避免显式循环，利用矩阵运算
- **内存访问优化**: 连续内存访问模式
- **并行计算**: 利用SIMD指令和GPU加速
- **数值稳定性**: 防止梯度爆炸或消失

**代码示例**:
```java
public class OptimizedGradientDescent {
    private final double learningRate;
    private final int batchSize;
    private final double momentum;

    public OptimizedGradientDescent(double learningRate, int batchSize, double momentum) {
        this.learningRate = learningRate;
        this.batchSize = batchSize;
        this.momentum = momentum;
    }

    // 向量化批量梯度下降
    public void trainBatch(double[][] X, double[][] y, double[][] weights,
                          double[][] bias, int epochs) {
        int numSamples = X.length;
        double[][] velocityW = new double[weights.length][weights[0].length];
        double[] velocityB = new double[bias[0].length];

        for (int epoch = 0; epoch < epochs; epoch++) {
            // 打乱数据
            int[] indices = shuffleIndices(numSamples);
            double totalLoss = 0.0;

            for (int batchStart = 0; batchStart < numSamples; batchStart += batchSize) {
                int batchEnd = Math.min(batchStart + batchSize, numSamples);
                int currentBatchSize = batchEnd - batchStart;

                // 准备批量数据
                double[][] batchX = new double[currentBatchSize][X[0].length];
                double[][] batchY = new double[currentBatchSize][y[0].length];

                for (int i = 0; i < currentBatchSize; i++) {
                    int idx = indices[batchStart + i];
                    System.arraycopy(X[idx], 0, batchX[i], 0, X[0].length);
                    System.arraycopy(y[idx], 0, batchY[i], 0, y[0].length);
                }

                // 前向传播（向量化）
                double[][] predictions = forwardPropagationBatch(batchX, weights, bias);

                // 计算损失
                double loss = calculateMSE(predictions, batchY);
                totalLoss += loss;

                // 计算梯度（向量化）
                Gradient gradients = calculateGradientsBatch(batchX, batchY, predictions);

                // 带动量的梯度更新
                updateParametersWithMomentum(weights, bias, gradients, velocityW, velocityB);
            }

            if (epoch % 100 == 0) {
                System.out.printf("Epoch %d, Loss: %.4f%n", epoch, totalLoss / numSamples);
            }
        }
    }

    // 向量化前向传播
    private double[][] forwardPropagationBatch(double[][] X, double[][] weights, double[][] bias) {
        // X: (batch_size, input_size)
        // weights: (input_size, output_size)
        // bias: (1, output_size)
        // return: (batch_size, output_size)

        double[][] output = new double[X.length][weights[0].length];

        // 向量化矩阵乘法
        for (int i = 0; i < X.length; i++) {
            for (int j = 0; j < weights[0].length; j++) {
                double sum = bias[0][j];
                for (int k = 0; k < X[0].length; k++) {
                    sum += X[i][k] * weights[k][j];
                }
                output[i][j] = sum;
            }
        }

        // 应用激活函数（例如ReLU）
        return applyActivation(output);
    }

    // 向量化梯度计算
    private Gradient calculateGradientsBatch(double[][] X, double[][] y, double[][] predictions) {
        int batchSize = X.length;
        int inputSize = X[0].length;
        int outputSize = y[0].length;

        // 计算输出层误差
        double[][] outputError = new double[batchSize][outputSize];
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < outputSize; j++) {
                outputError[i][j] = predictions[i][j] - y[i][j];
            }
        }

        // 计算权重梯度：dL/dW = X^T * outputError
        double[][] weightGradients = new double[inputSize][outputSize];
        for (int i = 0; i < inputSize; i++) {
            for (int j = 0; j < outputSize; j++) {
                double sum = 0.0;
                for (int k = 0; k < batchSize; k++) {
                    sum += X[k][i] * outputError[k][j];
                }
                weightGradients[i][j] = sum / batchSize;
            }
        }

        // 计算偏置梯度：dL/db = sum(outputError) / batch_size
        double[] biasGradients = new double[outputSize];
        for (int j = 0; j < outputSize; j++) {
            double sum = 0.0;
            for (int k = 0; k < batchSize; k++) {
                sum += outputError[k][j];
            }
            biasGradients[j] = sum / batchSize;
        }

        return new Gradient(weightGradients, biasGradients);
    }

    // 带动量的参数更新
    private void updateParametersWithMomentum(double[][] weights, double[][] bias,
                                             Gradient gradients, double[][] velocityW,
                                             double[] velocityB) {
        // 更新权重
        for (int i = 0; i < weights.length; i++) {
            for (int j = 0; j < weights[0].length; j++) {
                velocityW[i][j] = momentum * velocityW[i][j] - learningRate * gradients.weightGradients[i][j];
                weights[i][j] += velocityW[i][j];
            }
        }

        // 更新偏置
        for (int j = 0; j < bias[0].length; j++) {
            velocityB[j] = momentum * velocityB[j] - learningRate * gradients.biasGradients[j];
            bias[0][j] += velocityB[j];
        }
    }

    // 计算均方误差
    private double calculateMSE(double[][] predictions, double[][] targets) {
        double sum = 0.0;
        for (int i = 0; i < predictions.length; i++) {
            for (int j = 0; j < predictions[0].length; j++) {
                double error = predictions[i][j] - targets[i][j];
                sum += error * error;
            }
        }
        return sum / (predictions.length * predictions[0].length);
    }

    // 应用ReLU激活函数
    private double[][] applyActivation(double[][] x) {
        double[][] result = new double[x.length][x[0].length];
        for (int i = 0; i < x.length; i++) {
            for (int j = 0; j < x[0].length; j++) {
                result[i][j] = Math.max(0, x[i][j]);
            }
        }
        return result;
    }

    // 打乱索引
    private int[] shuffleIndices(int size) {
        int[] indices = new int[size];
        for (int i = 0; i < size; i++) {
            indices[i] = i;
        }

        Random random = new Random();
        for (int i = size - 1; i > 0; i--) {
            int j = random.nextInt(i + 1);
            int temp = indices[i];
            indices[i] = indices[j];
            indices[j] = temp;
        }

        return indices;
    }

    // 梯度数据结构
    private static class Gradient {
        final double[][] weightGradients;
        final double[] biasGradients;

        Gradient(double[][] weightGradients, double[] biasGradients) {
            this.weightGradients = weightGradients;
            this.biasGradients = biasGradients;
        }
    }
}
```

---

## 题目5: ⭐⭐⭐⭐⭐ 高级线性代数技术在生成对抗网络(GAN)中的应用

**问题描述**:
GAN的判别器和生成器都涉及复杂的线性代数运算。请分析矩阵运算在GAN训练中的作用，特别关注判别器中的特征映射和生成器中的噪声向量变换。

**答案要点**:
- **生成器**: 将噪声向量映射到数据空间
- **判别器**: 特征提取和分类决策
- **对抗训练**: 生成器和判别器的博弈过程
- **特征匹配**: 提高训练稳定性

**代码示例**:
```java
public class GANLinearAlgebra {
    private final int inputNoiseDim;
    private final int outputDim;
    private final int hiddenDim;

    private NeuralNetwork generator;
    private NeuralNetwork discriminator;

    public GANLinearAlgebra(int inputNoiseDim, int outputDim, int hiddenDim) {
        this.inputNoiseDim = inputNoiseDim;
        this.outputDim = outputDim;
        this.hiddenDim = hiddenDim;

        initializeNetworks();
    }

    private void initializeNetworks() {
        // 生成器：噪声 -> 隐藏层 -> 输出层
        this.generator = new NeuralNetwork(
            new int[]{inputNoiseDim, hiddenDim, outputDim},
            new String[]{"relu", "tanh"}
        );

        // 判别器：输入 -> 隐藏层 -> 输出层
        this.discriminator = new NeuralNetwork(
            new int[]{outputDim, hiddenDim, 1},
            new String[]{"relu", "sigmoid"}
        );
    }

    // 训练GAN
    public void train(double[][] realData, int epochs, int batchSize) {
        for (int epoch = 0; epoch < epochs; epoch++) {
            // 1. 训练判别器
            trainDiscriminator(realData, batchSize);

            // 2. 训练生成器
            trainGenerator(batchSize);

            if (epoch % 100 == 0) {
                double dLoss = calculateDiscriminatorLoss(realData, batchSize);
                double gLoss = calculateGeneratorLoss(batchSize);
                System.out.printf("Epoch %d: D_loss=%.4f, G_loss=%.4f%n", epoch, dLoss, gLoss);
            }
        }
    }

    // 训练判别器
    private void trainDiscriminator(double[][] realData, int batchSize) {
        // 准备真实数据
        double[][] realBatch = sampleBatch(realData, batchSize);
        double[][] realLabels = createLabels(batchSize, 1.0); // 真实标签为1

        // 生成假数据
        double[][] noise = generateNoise(batchSize);
        double[][] fakeData = generator.forward(noise);
        double[][] fakeLabels = createLabels(batchSize, 0.0); // 假标签为0

        // 合并数据进行训练
        double[][] combinedInput = combineBatches(realBatch, fakeData);
        double[][] combinedLabels = combineBatches(realLabels, fakeLabels);

        // 判别器前向传播
        double[][] predictions = discriminator.forward(combinedInput);

        // 计算判别器损失和梯度
        double[][] dLoss = calculateBinaryCrossEntropy(predictions, combinedLabels);
        double[][] dGradients = discriminator.backward(dLoss);

        // 更新判别器参数
        discriminator.updateParameters(0.001); // 学习率
    }

    // 训练生成器
    private void trainGenerator(int batchSize) {
        // 生成噪声
        double[][] noise = generateNoise(batchSize);

        // 生成器前向传播
        double[][] fakeData = generator.forward(noise);

        // 判别器前向传播（不要更新判别器参数）
        double[][] discriminatorOutput = discriminator.forward(fakeData);

        // 计算生成器损失：希望判别器将假数据判断为真
        double[][] targetLabels = createLabels(batchSize, 1.0);
        double[][] gLoss = calculateBinaryCrossEntropy(discriminatorOutput, targetLabels);

        // 计算梯度并回传到生成器
        double[][] discriminatorGradients = discriminator.backward(gLoss);
        double[][] generatorGradients = generator.backward(discriminatorGradients);

        // 更新生成器参数
        generator.updateParameters(0.001); // 学习率
    }

    // 生成新的样本
    public double[][] generate(int numSamples) {
        double[][] noise = generateNoise(numSamples);
        return generator.forward(noise);
    }

    // 生成随机噪声
    private double[][] generateNoise(int batchSize) {
        double[][] noise = new double[batchSize][inputNoiseDim];
        Random random = new Random();

        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < inputNoiseDim; j++) {
                noise[i][j] = random.nextGaussian(); // 标准正态分布
            }
        }

        return noise;
    }

    // 采样批量数据
    private double[][] sampleBatch(double[][] data, int batchSize) {
        double[][] batch = new double[batchSize][data[0].length];
        Random random = new Random();

        for (int i = 0; i < batchSize; i++) {
            int idx = random.nextInt(data.length);
            System.arraycopy(data[idx], 0, batch[i], 0, data[0].length);
        }

        return batch;
    }

    // 创建标签
    private double[][] createLabels(int batchSize, double value) {
        double[][] labels = new double[batchSize][1];
        for (int i = 0; i < batchSize; i++) {
            labels[i][0] = value;
        }
        return labels;
    }

    // 合并批次
    private double[][] combineBatches(double[][] batch1, double[][] batch2) {
        double[][] combined = new double[batch1.length + batch2.length][];
        System.arraycopy(batch1, 0, combined, 0, batch1.length);
        System.arraycopy(batch2, 0, combined, batch1.length, batch2.length);
        return combined;
    }

    // 计算二元交叉熵损失
    private double[][] calculateBinaryCrossEntropy(double[][] predictions, double[][] targets) {
        double[][] loss = new double[predictions.length][predictions[0].length];

        for (int i = 0; i < predictions.length; i++) {
            for (int j = 0; j < predictions[0].length; j++) {
                double p = Math.max(Math.min(predictions[i][j], 1e-15), 1 - 1e-15);
                loss[i][j] = -(targets[i][j] * Math.log(p) + (1 - targets[i][j]) * Math.log(1 - p));
            }
        }

        return loss;
    }

    // 计算判别器损失
    private double calculateDiscriminatorLoss(double[][] realData, int batchSize) {
        // 真实数据损失
        double[][] realBatch = sampleBatch(realData, batchSize);
        double[][] realLabels = createLabels(batchSize, 1.0);
        double[][] realPred = discriminator.forward(realBatch);
        double realLoss = calculateBinaryCrossEntropy(realPred, realLabels);

        // 假数据损失
        double[][] noise = generateNoise(batchSize);
        double[][] fakeData = generator.forward(noise);
        double[][] fakeLabels = createLabels(batchSize, 0.0);
        double[][] fakePred = discriminator.forward(fakeData);
        double fakeLoss = calculateBinaryCrossEntropy(fakePred, fakeLabels);

        // 计算总损失
        double totalLoss = 0.0;
        for (int i = 0; i < realLoss.length; i++) {
            for (int j = 0; j < realLoss[0].length; j++) {
                totalLoss += realLoss[i][j] + fakeLoss[i][j];
            }
        }

        return totalLoss / (batchSize * 2);
    }

    // 计算生成器损失
    private double calculateGeneratorLoss(int batchSize) {
        double[][] noise = generateNoise(batchSize);
        double[][] fakeData = generator.forward(noise);
        double[][] targetLabels = createLabels(batchSize, 1.0);
        double[][] predictions = discriminator.forward(fakeData);
        double[][] loss = calculateBinaryCrossEntropy(predictions, targetLabels);

        double totalLoss = 0.0;
        for (int i = 0; i < loss.length; i++) {
            for (int j = 0; j < loss[0].length; j++) {
                totalLoss += loss[i][j];
            }
        }

        return totalLoss / batchSize;
    }

    // 简化的神经网络实现
    private static class NeuralNetwork {
        private final int[] layerSizes;
        private final String[] activations;
        private double[][][] weights;
        private double[][] biases;

        public NeuralNetwork(int[] layerSizes, String[] activations) {
            this.layerSizes = layerSizes;
            this.activations = activations;
            this.weights = new double[layerSizes.length - 1][][];
            this.biases = new double[layerSizes.length - 1][];

            initializeParameters();
        }

        private void initializeParameters() {
            Random random = new Random();

            for (int i = 0; i < layerSizes.length - 1; i++) {
                weights[i] = new double[layerSizes[i]][layerSizes[i + 1]];
                biases[i] = new double[layerSizes[i + 1]];

                // Xavier初始化
                double scale = Math.sqrt(2.0 / (layerSizes[i] + layerSizes[i + 1]));

                for (int j = 0; j < layerSizes[i]; j++) {
                    for (int k = 0; k < layerSizes[i + 1]; k++) {
                        weights[i][j][k] = random.nextGaussian() * scale;
                    }
                }

                for (int k = 0; k < layerSizes[i + 1]; k++) {
                    biases[i][k] = 0.0;
                }
            }
        }

        public double[][] forward(double[][] input) {
            double[][] current = input;

            for (int layer = 0; layer < weights.length; layer++) {
                // 矩阵乘法: current * weights[layer] + biases[layer]
                double[][] next = new double[current.length][weights[layer][0].length];

                for (int i = 0; i < current.length; i++) {
                    for (int j = 0; j < weights[layer][0].length; j++) {
                        double sum = biases[layer][j];
                        for (int k = 0; k < current[0].length; k++) {
                            sum += current[i][k] * weights[layer][k][j];
                        }
                        next[i][j] = sum;
                    }
                }

                // 应用激活函数
                next = applyActivation(next, activations[layer]);
                current = next;
            }

            return current;
        }

        public double[][] backward(double[][] gradient) {
            // 简化的反向传播实现
            return gradient;
        }

        public void updateParameters(double learningRate) {
            // 简化的参数更新实现
            for (int i = 0; i < weights.length; i++) {
                for (int j = 0; j < weights[i].length; j++) {
                    for (int k = 0; k < weights[i][j].length; k++) {
                        weights[i][j][k] -= learningRate * 0.01; // 简化实现
                    }
                }
            }
        }

        private double[][] applyActivation(double[][] input, String activation) {
            double[][] output = new double[input.length][input[0].length];

            for (int i = 0; i < input.length; i++) {
                for (int j = 0; j < input[0].length; j++) {
                    switch (activation.toLowerCase()) {
                        case "relu":
                            output[i][j] = Math.max(0, input[i][j]);
                            break;
                        case "tanh":
                            output[i][j] = Math.tanh(input[i][j]);
                            break;
                        case "sigmoid":
                            output[i][j] = 1.0 / (1.0 + Math.exp(-input[i][j]));
                            break;
                        default:
                            output[i][j] = input[i][j];
                    }
                }
            }

            return output;
        }
    }
}
```

---

**总结**: 线性代数是AI和机器学习的数学基础，从基础的矩阵运算到高级的张量分解，在各种AI算法中都发挥着关键作用。掌握线性代数的Java实现对于构建高效的AI系统至关重要。在实际开发中，应该充分利用现有的数值计算库来提高性能和准确性。