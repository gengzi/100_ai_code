# 概率论与统计在机器学习中的应用

## 题目1: ⭐⭐ 贝叶斯定理在朴素贝叶斯分类器中的实现

**问题描述**:
朴素贝叶斯分类器是基于贝叶斯定理的概率分类算法。请解释贝叶斯定理在文本分类中的应用，并用Java实现一个多项式朴素贝叶斯分类器。

**答案要点**:
- **贝叶斯定理**: P(y|x) = P(x|y) * P(y) / P(x)
- **特征独立性假设**: 简化计算复杂度
- **拉普拉斯平滑**: 避免零概率问题
- **对数概率**: 防止数值下溢

**代码示例**:
```java
public class NaiveBayesClassifier {
    private final Map<String, Double> classPriors = new HashMap<>();
    private final Map<String, Map<String, Integer>> featureCounts = new HashMap<>();
    private final Map<String, Integer> classTotalFeatures = new HashMap<>();
    private final Set<String> vocabulary = new HashSet<>();
    private final double smoothingFactor = 1.0; // 拉普拉斯平滑

    public void train(List<Document> documents, List<String> labels) {
        if (documents.size() != labels.size()) {
            throw new IllegalArgumentException("文档和标签数量不匹配");
        }

        // 1. 统计类别先验概率
        calculateClassPriors(labels);

        // 2. 统计特征和类别的联合概率
        calculateFeatureProbabilities(documents, labels);

        // 3. 构建词汇表
        buildVocabulary(documents);
    }

    public String predict(Document document) {
        Map<String, Double> logProbabilities = new HashMap<>();

        // 计算每个类别的后验概率（对数空间）
        for (String className : classPriors.keySet()) {
            double logProb = Math.log(classPriors.get(className));

            // 计算给定类别下文档的概率
            for (String word : document.getWords()) {
                if (vocabulary.contains(word)) {
                    double wordProb = calculateWordProbability(word, className);
                    logProb += Math.log(wordProb);
                }
            }

            logProbabilities.put(className, logProb);
        }

        // 返回概率最高的类别
        return logProbabilities.entrySet().stream()
            .max(Map.Entry.comparingByValue())
            .map(Map.Entry::getKey)
            .orElse(null);
    }

    public Map<String, Double> predictProbabilities(Document document) {
        Map<String, Double> probabilities = new HashMap<>();
        Map<String, Double> logProbabilities = new HashMap<>();

        // 计算每个类别的对数概率
        for (String className : classPriors.keySet()) {
            double logProb = Math.log(classPriors.get(className));

            for (String word : document.getWords()) {
                if (vocabulary.contains(word)) {
                    double wordProb = calculateWordProbability(word, className);
                    logProb += Math.log(wordProb);
                }
            }
            logProbabilities.put(className, logProb);
        }

        // 对数概率转换为实际概率（使用log-sum-exp技巧）
        double maxLogProb = logProbabilities.values().stream()
            .mapToDouble(Double::doubleValue)
            .max()
            .orElse(Double.NEGATIVE_INFINITY);

        double sumExp = logProbabilities.values().stream()
            .mapToDouble(logP -> Math.exp(logP - maxLogProb))
            .sum();

        for (Map.Entry<String, Double> entry : logProbabilities.entrySet()) {
            double prob = Math.exp(entry.getValue() - maxLogProb) / sumExp;
            probabilities.put(entry.getKey(), prob);
        }

        return probabilities;
    }

    private void calculateClassPriors(List<String> labels) {
        int totalDocuments = labels.size();
        Map<String, Integer> classCounts = new HashMap<>();

        for (String label : labels) {
            classCounts.put(label, classCounts.getOrDefault(label, 0) + 1);
        }

        for (Map.Entry<String, Integer> entry : classCounts.entrySet()) {
            double prior = (double) entry.getValue() / totalDocuments;
            classPriors.put(entry.getKey(), prior);
        }
    }

    private void calculateFeatureProbabilities(List<Document> documents, List<String> labels) {
        for (int i = 0; i < documents.size(); i++) {
            String className = labels.get(i);
            Document document = documents.get(i);

            // 初始化类别的特征计数
            featureCounts.putIfAbsent(className, new HashMap<>());

            for (String word : document.getWords()) {
                // 增加该词在当前类别中的计数
                Map<String, Integer> wordCounts = featureCounts.get(className);
                wordCounts.put(word, wordCounts.getOrDefault(word, 0) + 1);

                // 更新类别总特征数
                classTotalFeatures.put(className,
                    classTotalFeatures.getOrDefault(className, 0) + 1);
            }
        }
    }

    private void buildVocabulary(List<Document> documents) {
        for (Document document : documents) {
            vocabulary.addAll(document.getWords());
        }
    }

    private double calculateWordProbability(String word, String className) {
        int wordCount = featureCounts.getOrDefault(className, Collections.emptyMap())
            .getOrDefault(word, 0);
        int totalWords = classTotalFeatures.getOrDefault(className, 0);
        int vocabularySize = vocabulary.size();

        // 拉普拉斯平滑
        return (wordCount + smoothingFactor) / (totalWords + smoothingFactor * vocabularySize);
    }

    // 文档数据结构
    public static class Document {
        private final String text;
        private final List<String> words;

        public Document(String text) {
            this.text = text;
            this.words = tokenize(text);
        }

        private List<String> tokenize(String text) {
            // 简单的分词实现
            String[] tokens = text.toLowerCase()
                .replaceAll("[^a-z0-9\\s]", "")
                .split("\\s+");
            return Arrays.asList(tokens);
        }

        public String getText() { return text; }
        public List<String> getWords() { return words; }
    }
}
```

---

## 题目2: ⭐⭐⭐ 高斯分布与最大似然估计

**问题描述**:
在机器学习中，很多算法假设数据服从高斯分布。请解释最大似然估计在参数估计中的应用，并用Java实现高斯分布的参数估计和概率密度计算。

**答案要点**:
- **高斯分布**: 正态分布的概率密度函数
- **最大似然估计**: 最大化观测数据的似然函数
- **参数估计**: 均值和方差的闭式解
- **数值稳定性**: 对数似然函数的使用

**代码示例**:
```java
public class GaussianDistribution {
    private double mean;
    private double variance;
    private double standardDeviation;

    public GaussianDistribution(double mean, double variance) {
        this.mean = mean;
        this.variance = variance;
        this.standardDeviation = Math.sqrt(variance);
    }

    // 从数据中估计参数（最大似然估计）
    public static GaussianDistribution fit(double[] data) {
        if (data.length == 0) {
            throw new IllegalArgumentException("数据不能为空");
        }

        // 计算均值
        double sum = 0.0;
        for (double value : data) {
            sum += value;
        }
        double estimatedMean = sum / data.length;

        // 计算方差（无偏估计）
        double sumSquaredDifferences = 0.0;
        for (double value : data) {
            double diff = value - estimatedMean;
            sumSquaredDifferences += diff * diff;
        }
        double estimatedVariance = sumSquaredDifferences / (data.length - 1);

        return new GaussianDistribution(estimatedMean, estimatedVariance);
    }

    // 计算概率密度
    public double probabilityDensity(double x) {
        double exponent = -Math.pow(x - mean, 2) / (2 * variance);
        return (1.0 / (standardDeviation * Math.sqrt(2 * Math.PI))) * Math.exp(exponent);
    }

    // 计算对数概率密度（数值稳定）
    public double logProbabilityDensity(double x) {
        double logCoefficient = -Math.log(standardDeviation) - 0.5 * Math.log(2 * Math.PI);
        double logExponent = -Math.pow(x - mean, 2) / (2 * variance);
        return logCoefficient + logExponent;
    }

    // 计算累积分布函数
    public double cumulativeDistribution(double x) {
        return 0.5 * (1 + erf((x - mean) / (standardDeviation * Math.sqrt(2))));
    }

    // 计算对数似然
    public double logLikelihood(double[] data) {
        double sumLogProb = 0.0;
        for (double value : data) {
            sumLogProb += logProbabilityDensity(value);
        }
        return sumLogProb;
    }

    // 计算分位数
    public double quantile(double p) {
        return mean + standardDeviation * Math.sqrt(2) * inverseErrorFunction(2 * p - 1);
    }

    // 生成随机样本
    public double sample() {
        Random random = new Random();
        return mean + standardDeviation * random.nextGaussian();
    }

    public double[] sample(int n) {
        double[] samples = new double[n];
        Random random = new Random();
        for (int i = 0; i < n; i++) {
            samples[i] = mean + standardDeviation * random.nextGaussian();
        }
        return samples;
    }

    // 计算KL散度
    public double klDivergence(GaussianDistribution other) {
        double logRatio = Math.log(other.standardDeviation / standardDeviation);
        double varianceRatio = variance / other.variance;
        double meanDifference = Math.pow(mean - other.mean, 2) / other.variance;

        return logRatio + (varianceRatio + meanDifference - 1) / 2;
    }

    // 误差函数的近似实现
    private static double erf(double x) {
        // 使用Abramowitz和Stegun的近似公式
        double a1 =  0.254829592;
        double a2 = -0.284496736;
        double a3 =  1.421413741;
        double a4 = -1.453152027;
        double a5 =  1.061405429;
        double p  =  0.3275911;

        int sign = x < 0 ? -1 : 1;
        x = Math.abs(x);

        double t = 1.0 / (1.0 + p * x);
        double t2 = t * t;
        double t3 = t2 * t;
        double t4 = t3 * t;
        double t5 = t4 * t;

        double y = 1.0 - (((((a5 * t5 + a4 * t4) + a3 * t3) + a2 * t2) + a1 * t) * Math.exp(-x * x));

        return sign * y;
    }

    // 逆误差函数的近似实现
    private static double inverseErrorFunction(double y) {
        double a = 0.147;
        double ln1y2 = Math.log(1 - y * y);
        double term1 = 2 / (Math.PI * a) + ln1y2 / 2;
        double term2 = ln1y2 / a;

        return Math.signum(y) * Math.sqrt(Math.sqrt(term1 * term1 - term2) - term1);
    }

    // Getters
    public double getMean() { return mean; }
    public double getVariance() { return variance; }
    public double getStandardDeviation() { return standardDeviation; }
}

// 多元高斯分布
public class MultivariateGaussian {
    private final double[] mean;
    private final double[][] covariance;
    private final double[][] precision; // 协方差矩阵的逆
    private final double logDeterminant;
    private final int dimension;

    public MultivariateGaussian(double[] mean, double[][] covariance) {
        this.mean = mean.clone();
        this.covariance = cloneMatrix(covariance);
        this.dimension = mean.length;
        this.precision = matrixInverse(covariance);
        this.logDeterminant = Math.log(matrixDeterminant(covariance));
    }

    public double probabilityDensity(double[] x) {
        double[] diff = subtractVectors(x, mean);
        double exponent = -0.5 * quadraticForm(diff, precision);
        double normalization = Math.pow(2 * Math.PI, -dimension / 2.0) * Math.exp(-0.5 * logDeterminant);

        return normalization * Math.exp(exponent);
    }

    public double logProbabilityDensity(double[] x) {
        double[] diff = subtractVectors(x, mean);
        double quadratic = quadraticForm(diff, precision);
        double logNormalization = -dimension / 2.0 * Math.log(2 * Math.PI) - 0.5 * logDeterminant;

        return logNormalization - 0.5 * quadratic;
    }

    // 辅助矩阵运算方法
    private double[] subtractVectors(double[] a, double[] b) {
        double[] result = new double[a.length];
        for (int i = 0; i < a.length; i++) {
            result[i] = a[i] - b[i];
        }
        return result;
    }

    private double quadraticForm(double[] vector, double[][] matrix) {
        double result = 0.0;
        for (int i = 0; i < vector.length; i++) {
            for (int j = 0; j < vector.length; j++) {
                result += vector[i] * matrix[i][j] * vector[j];
            }
        }
        return result;
    }

    private double[][] cloneMatrix(double[][] matrix) {
        double[][] result = new double[matrix.length][matrix[0].length];
        for (int i = 0; i < matrix.length; i++) {
            System.arraycopy(matrix[i], 0, result[i], 0, matrix[0].length);
        }
        return result;
    }

    private double[][] matrixInverse(double[][] matrix) {
        // 简化实现：实际应用中应使用专业的线性代数库
        int n = matrix.length;
        double[][] augmented = new double[n][2 * n];

        // 构建增广矩阵
        for (int i = 0; i < n; i++) {
            System.arraycopy(matrix[i], 0, augmented[i], 0, n);
            augmented[i][i + n] = 1.0;
        }

        // 高斯-约旦消元法
        for (int i = 0; i < n; i++) {
            // 主元归一化
            double pivot = augmented[i][i];
            for (int j = 0; j < 2 * n; j++) {
                augmented[i][j] /= pivot;
            }

            // 消元
            for (int k = 0; k < n; k++) {
                if (k != i) {
                    double factor = augmented[k][i];
                    for (int j = 0; j < 2 * n; j++) {
                        augmented[k][j] -= factor * augmented[i][j];
                    }
                }
            }
        }

        // 提取逆矩阵
        double[][] inverse = new double[n][n];
        for (int i = 0; i < n; i++) {
            System.arraycopy(augmented[i], n, inverse[i], 0, n);
        }

        return inverse;
    }

    private double matrixDeterminant(double[][] matrix) {
        // 简化实现：使用LU分解计算行列式
        int n = matrix.length;
        double[][] temp = cloneMatrix(matrix);
        double det = 1.0;

        for (int i = 0; i < n; i++) {
            // 寻找主元
            int pivot = i;
            while (pivot < n && Math.abs(temp[pivot][i]) < 1e-10) {
                pivot++;
            }

            if (pivot == n) return 0.0;

            if (pivot != i) {
                // 交换行
                double[] tempRow = temp[i];
                temp[i] = temp[pivot];
                temp[pivot] = tempRow;
                det = -det;
            }

            det *= temp[i][i];

            // 消元
            for (int k = i + 1; k < n; k++) {
                double factor = temp[k][i] / temp[i][i];
                for (int j = i; j < n; j++) {
                    temp[k][j] -= factor * temp[i][j];
                }
            }
        }

        return det;
    }
}
```

---

## 题目3: ⭐⭐⭐⭐ 混合高斯模型与EM算法

**问题描述**:
混合高斯模型(GMM)是概率聚类和密度估计的重要方法。请解释EM算法在GMM参数估计中的作用，并用Java实现一个完整的GMM训练和预测系统。

**答案要点**:
- **GMM模型**: 多个高斯分布的加权组合
- **EM算法**: E步计算后验概率，M步最大化期望
- **初始化策略**: K-means初始化提高收敛性
- **收敛判断**: 监测对数似然函数的变化

**代码示例**:
```java
public class GaussianMixtureModel {
    private int numComponents;
    private GaussianDistribution[] components;
    private double[] weights;
    private final double convergenceThreshold = 1e-6;
    private final int maxIterations = 1000;

    public GaussianMixtureModel(int numComponents) {
        this.numComponents = numComponents;
    }

    // 训练GMM模型
    public void fit(double[][] data) {
        int numSamples = data.length;
        int dimension = data[0].length;

        // 1. 初始化参数
        initializeParameters(data);

        double previousLogLikelihood = Double.NEGATIVE_INFINITY;

        for (int iteration = 0; iteration < maxIterations; iteration++) {
            // E步：计算后验概率（responsibilities）
            double[][] responsibilities = calculateResponsibilities(data);

            // M步：更新参数
            updateParameters(data, responsibilities);

            // 计算对数似然
            double currentLogLikelihood = calculateLogLikelihood(data);

            // 检查收敛
            if (Math.abs(currentLogLikelihood - previousLogLikelihood) < convergenceThreshold) {
                System.out.printf("GMM收敛于第 %d 次迭代，对数似然: %.6f%n",
                                 iteration, currentLogLikelihood);
                break;
            }

            previousLogLikelihood = currentLogLikelihood;

            if (iteration % 10 == 0) {
                System.out.printf("迭代 %d: 对数似然 = %.6f%n", iteration, currentLogLikelihood);
            }
        }
    }

    // 预测样本属于哪个组件
    public int predict(double[] sample) {
        double[] posterior = posteriorProbabilities(sample);
        int bestComponent = 0;
        double maxProbability = posterior[0];

        for (int i = 1; i < posterior.length; i++) {
            if (posterior[i] > maxProbability) {
                maxProbability = posterior[i];
                bestComponent = i;
            }
        }

        return bestComponent;
    }

    // 计算后验概率
    public double[] posteriorProbabilities(double[] sample) {
        double[] posterior = new double[numComponents];
        double totalProbability = 0.0;

        // 计算每个组件的非归一化后验概率
        for (int k = 0; k < numComponents; k++) {
            posterior[k] = weights[k] * components[k].probabilityDensity(sample);
            totalProbability += posterior[k];
        }

        // 归一化
        if (totalProbability > 1e-10) {
            for (int k = 0; k < numComponents; k++) {
                posterior[k] /= totalProbability;
            }
        } else {
            // 如果所有概率都很小，均匀分配
            for (int k = 0; k < numComponents; k++) {
                posterior[k] = 1.0 / numComponents;
            }
        }

        return posterior;
    }

    // 计算样本的密度
    public double probabilityDensity(double[] sample) {
        double density = 0.0;
        for (int k = 0; k < numComponents; k++) {
            density += weights[k] * components[k].probabilityDensity(sample);
        }
        return density;
    }

    // 使用K-means初始化参数
    private void initializeParameters(double[][] data) {
        // 1. 使用K-means聚类获得初始中心
        KMeans kmeans = new KMeans(numComponents);
        kmeans.fit(data);
        List<double[]> centroids = kmeans.getCentroids();
        List<List<double[]>> clusters = kmeans.getClusters();

        components = new GaussianDistribution[numComponents];
        weights = new double[numComponents];
        int totalSamples = data.length;

        // 2. 为每个簇初始化高斯分布
        for (int k = 0; k < numComponents; k++) {
            List<double[]> clusterData = clusters.get(k);

            if (!clusterData.isEmpty()) {
                // 将多维数据投影到一维（简化处理）
                double[] oneDimData = projectToOneDimension(clusterData);
                components[k] = GaussianDistribution.fit(oneDimData);
                weights[k] = (double) clusterData.size() / totalSamples;
            } else {
                // 处理空簇
                components[k] = new GaussianDistribution(0.0, 1.0);
                weights[k] = 1.0 / numComponents;
            }
        }
    }

    // 将多维数据投影到一维（使用主成分）
    private double[] projectToOneDimension(List<double[]> clusterData) {
        if (clusterData.isEmpty()) {
            return new double[0];
        }

        int dimension = clusterData.get(0).length;
        double[] projected = new double[clusterData.size()];

        // 简单投影：使用第一个特征
        for (int i = 0; i < clusterData.size(); i++) {
            projected[i] = clusterData.get(i)[0];
        }

        return projected;
    }

    // E步：计算responsibilities
    private double[][] calculateResponsibilities(double[][] data) {
        int numSamples = data.length;
        double[][] responsibilities = new double[numSamples][numComponents];

        for (int i = 0; i < numSamples; i++) {
            double[] posterior = posteriorProbabilities(data[i]);
            System.arraycopy(posterior, 0, responsibilities[i], 0, numComponents);
        }

        return responsibilities;
    }

    // M步：更新参数
    private void updateParameters(double[][] data, double[][] responsibilities) {
        int numSamples = data.length;
        int dimension = data[0].length;

        // 计算每个组件的有效样本数
        double[] effectiveSampleSizes = new double[numComponents];
        for (int k = 0; k < numComponents; k++) {
            for (int i = 0; i < numSamples; i++) {
                effectiveSampleSizes[k] += responsibilities[i][k];
            }
        }

        // 更新权重
        for (int k = 0; k < numComponents; k++) {
            weights[k] = effectiveSampleSizes[k] / numSamples;
        }

        // 更新每个高斯分布的参数
        for (int k = 0; k < numComponents; k++) {
            if (effectiveSampleSizes[k] > 1e-10) {
                // 投影到一维并更新参数
                double[] weightedData = calculateWeightedMeanAndVariance(data, responsibilities, k);
                components[k] = new GaussianDistribution(weightedData[0], weightedData[1]);
            }
        }
    }

    // 计算加权均值和方差
    private double[] calculateWeightedMeanAndVariance(double[][] data,
                                                     double[][] responsibilities, int component) {
        double weightedSum = 0.0;
        double weightedSumSquared = 0.0;
        double totalWeight = 0.0;

        for (int i = 0; i < data.length; i++) {
            double weight = responsibilities[i][component];
            double value = data[i][0]; // 使用第一个特征
            weightedSum += weight * value;
            weightedSumSquared += weight * value * value;
            totalWeight += weight;
        }

        if (totalWeight > 1e-10) {
            double mean = weightedSum / totalWeight;
            double variance = (weightedSumSquared / totalWeight) - mean * mean;
            variance = Math.max(variance, 1e-6); // 防止方差过小
            return new double[]{mean, variance};
        } else {
            return new double[]{0.0, 1.0};
        }
    }

    // 计算对数似然
    private double calculateLogLikelihood(double[][] data) {
        double logLikelihood = 0.0;

        for (double[] sample : data) {
            double density = probabilityDensity(sample);
            if (density > 1e-10) {
                logLikelihood += Math.log(density);
            }
        }

        return logLikelihood;
    }

    // 简化的K-means实现
    private static class KMeans {
        private final int numClusters;
        private List<double[]> centroids;
        private List<List<double[]>> clusters;

        public KMeans(int numClusters) {
            this.numClusters = numClusters;
        }

        public void fit(double[][] data) {
            initializeCentroids(data);

            for (int iteration = 0; iteration < 100; iteration++) {
                assignToClusters(data);
                updateCentroids();
            }
        }

        private void initializeCentroids(double[][] data) {
            centroids = new ArrayList<>();
            Random random = new Random();
            Set<Integer> usedIndices = new HashSet<>();

            while (centroids.size() < numClusters) {
                int index = random.nextInt(data.length);
                if (usedIndices.add(index)) {
                    centroids.add(data[index].clone());
                }
            }
        }

        private void assignToClusters(double[][] data) {
            clusters = new ArrayList<>();
            for (int i = 0; i < numClusters; i++) {
                clusters.add(new ArrayList<>());
            }

            for (double[] point : data) {
                int closestCluster = findClosestCluster(point);
                clusters.get(closestCluster).add(point);
            }
        }

        private int findClosestCluster(double[] point) {
            int closest = 0;
            double minDistance = Double.MAX_VALUE;

            for (int i = 0; i < centroids.size(); i++) {
                double distance = euclideanDistance(point, centroids.get(i));
                if (distance < minDistance) {
                    minDistance = distance;
                    closest = i;
                }
            }

            return closest;
        }

        private double euclideanDistance(double[] a, double[] b) {
            double sum = 0.0;
            for (int i = 0; i < a.length; i++) {
                double diff = a[i] - b[i];
                sum += diff * diff;
            }
            return Math.sqrt(sum);
        }

        private void updateCentroids() {
            for (int i = 0; i < numClusters; i++) {
                List<double[]> cluster = clusters.get(i);
                if (!cluster.isEmpty()) {
                    double[] newCentroid = new double[cluster.get(0).length];
                    for (double[] point : cluster) {
                        for (int j = 0; j < point.length; j++) {
                            newCentroid[j] += point[j];
                        }
                    }
                    for (int j = 0; j < newCentroid.length; j++) {
                        newCentroid[j] /= cluster.size();
                    }
                    centroids.set(i, newCentroid);
                }
            }
        }

        public List<double[]> getCentroids() { return centroids; }
        public List<List<double[]>> getClusters() { return clusters; }
    }

    // Getters
    public GaussianDistribution[] getComponents() { return components; }
    public double[] getWeights() { return weights; }
    public int getNumComponents() { return numComponents; }
}
```

---

## 题目4: ⭐⭐⭐⭐⭐ 贝叶斯网络与概率推理

**问题描述**:
贝叶斯网络是一种概率图模型，用于表示变量间的条件依赖关系。请设计一个Java实现的简单贝叶斯网络，支持概率推理和学习。

**答案要点**:
- **网络结构**: 有向无环图表示变量关系
- **条件概率表**: 存储父子节点的条件概率
- **概率推理**: 变量消元或采样推理
- **参数学习**: 从数据中学习CPT参数

**代码示例**:
```java
public class BayesianNetwork {
    private final Map<String, Variable> variables = new HashMap<>();
    private final Map<String, List<String>> adjacencyList = new HashMap<>();
    private final Map<String, ConditionalProbabilityTable> cpts = new HashMap<>();

    // 添加变量到网络
    public void addVariable(String name, List<String> states) {
        Variable variable = new Variable(name, states);
        variables.put(name, variable);
        adjacencyList.put(name, new ArrayList<>());
    }

    // 添加有向边
    public void addEdge(String parent, String child) {
        if (!variables.containsKey(parent) || !variables.containsKey(child)) {
            throw new IllegalArgumentException("变量不存在");
        }

        // 检查是否会形成环
        if (wouldCreateCycle(parent, child)) {
            throw new IllegalArgumentException("添加边会形成环");
        }

        adjacencyList.get(parent).add(child);
    }

    // 设置条件概率表
    public void setCPT(String variable, ConditionalProbabilityTable cpt) {
        cpts.put(variable, cpt);
    }

    // 计算联合概率
    public double jointProbability(Map<String, String> assignment) {
        double probability = 1.0;

        for (String variableName : variables.keySet()) {
            Variable variable = variables.get(variableName);
            String state = assignment.get(variableName);

            if (state == null) {
                throw new IllegalArgumentException("缺少变量 " + variableName + " 的赋值");
            }

            List<String> parents = adjacencyList.get(variableName);
            Map<String, String> parentAssignment = new HashMap<>();

            for (String parent : parents) {
                parentAssignment.put(parent, assignment.get(parent));
            }

            ConditionalProbabilityTable cpt = cpts.get(variableName);
            probability *= cpt.getProbability(state, parentAssignment);
        }

        return probability;
    }

    // 变量消元推理
    public double variableElimination(Map<String, String> evidence, String queryVariable, String queryState) {
        // 构建因子列表
        List<Factor> factors = buildFactors(evidence);

        // 消元顺序（简化：按字母顺序）
        List<String> eliminationOrder = getEliminationOrder(queryVariable, evidence.keySet());

        // 变量消元过程
        for (String variable : eliminationOrder) {
            factors = eliminateVariable(factors, variable);
        }

        // 计算最终概率
        Factor finalFactor = multiplyFactors(factors);
        double numerator = finalFactor.getValue(Map.of(queryVariable, queryState));
        double denominator = 0.0;

        for (String state : variables.get(queryVariable).getStates()) {
            denominator += finalFactor.getValue(Map.of(queryVariable, state));
        }

        return numerator / denominator;
    }

    // 近似推理：Gibbs采样
    public Map<String, Double> gibbsSampling(Map<String, String> evidence, String queryVariable,
                                            int numIterations, int burnIn) {
        // 初始化采样状态
        Map<String, String> currentState = initializeSamplingState(evidence);

        Map<String, Integer> counts = new HashMap<>();
        for (String state : variables.get(queryVariable).getStates()) {
            counts.put(state, 0);
        }

        // Gibbs采样
        for (int i = 0; i < numIterations + burnIn; i++) {
            for (String variable : variables.keySet()) {
                if (!evidence.containsKey(variable)) {
                    // 从条件分布中采样
                    String newState = sampleVariable(variable, currentState, evidence);
                    currentState.put(variable, newState);
                }
            }

            // 记录采样结果（跳过burn-in期）
            if (i >= burnIn) {
                String currentStateValue = currentState.get(queryVariable);
                counts.put(currentStateValue, counts.get(currentStateValue) + 1);
            }
        }

        // 计算概率估计
        Map<String, Double> probabilities = new HashMap<>();
        int totalSamples = numIterations;
        for (Map.Entry<String, Integer> entry : counts.entrySet()) {
            probabilities.put(entry.getKey(), (double) entry.getValue() / totalSamples);
        }

        return probabilities;
    }

    // 从数据中学习参数
    public void learnParameters(List<Map<String, String>> data) {
        for (String variableName : variables.keySet()) {
            List<String> parents = adjacencyList.get(variableName);
            ConditionalProbabilityTable cpt = new ConditionalProbabilityTable(variableName, parents);

            // 统计计数
            Map<String, Integer> counts = new HashMap<>();
            Map<String, Integer> parentCounts = new HashMap<>();

            for (Map<String, String> sample : data) {
                String state = sample.get(variableName);
                String parentConfig = getParentConfiguration(sample, parents);

                String key = state + "|" + parentConfig;
                counts.put(key, counts.getOrDefault(key, 0) + 1);
                parentCounts.put(parentConfig, parentCounts.getOrDefault(parentConfig, 0) + 1);
            }

            // 计算概率
            for (String state : variables.get(variableName).getStates()) {
                for (String parentConfig : getAllParentConfigurations(parents)) {
                    String key = state + "|" + parentConfig;
                    int count = counts.getOrDefault(key, 0);
                    int parentCount = parentCounts.getOrDefault(parentConfig, 1); // 避免除零

                    double probability = (double) count / parentCount;
                    cpt.setProbability(state, parseParentConfig(parentConfig), probability);
                }
            }

            cpts.put(variableName, cpt);
        }
    }

    // 辅助方法
    private boolean wouldCreateCycle(String parent, String child) {
        // 使用DFS检查是否会形成环
        Set<String> visited = new HashSet<>();
        return hasCycleDFS(child, parent, visited);
    }

    private boolean hasCycleDFS(String current, String target, Set<String> visited) {
        if (current.equals(target)) {
            return true;
        }

        if (visited.contains(current)) {
            return false;
        }

        visited.add(current);
        for (String neighbor : adjacencyList.get(current)) {
            if (hasCycleDFS(neighbor, target, visited)) {
                return true;
            }
        }

        return false;
    }

    private List<Factor> buildFactors(Map<String, String> evidence) {
        List<Factor> factors = new ArrayList<>();

        for (String variableName : variables.keySet()) {
            ConditionalProbabilityTable cpt = cpts.get(variableName);
            Factor factor = new Factor(variableName, adjacencyList.get(variableName));

            // 应用证据
            for (Map.Entry<String, String> e : evidence.entrySet()) {
                if (factor.getVariables().contains(e.getKey())) {
                    factor.reduce(e.getKey(), e.getValue());
                }
            }

            factors.add(factor);
        }

        return factors;
    }

    private List<String> getEliminationOrder(String queryVariable, Set<String> evidenceVariables) {
        List<String> order = new ArrayList<>();

        for (String variable : variables.keySet()) {
            if (!variable.equals(queryVariable) && !evidenceVariables.contains(variable)) {
                order.add(variable);
            }
        }

        Collections.sort(order);
        return order;
    }

    private List<Factor> eliminateVariable(List<Factor> factors, String variable) {
        List<Factor> relevantFactors = new ArrayList<>();
        List<Factor> remainingFactors = new ArrayList<>();

        for (Factor factor : factors) {
            if (factor.getVariables().contains(variable)) {
                relevantFactors.add(factor);
            } else {
                remainingFactors.add(factor);
            }
        }

        if (relevantFactors.isEmpty()) {
            return factors;
        }

        // 乘法
        Factor product = relevantFactors.get(0);
        for (int i = 1; i < relevantFactors.size(); i++) {
            product = product.multiply(relevantFactors.get(i));
        }

        // 消元（边缘化）
        Factor marginalized = product.marginalize(variable);
        remainingFactors.add(marginalized);

        return remainingFactors;
    }

    private Factor multiplyFactors(List<Factor> factors) {
        if (factors.isEmpty()) {
            return new Factor(Collections.emptyList());
        }

        Factor result = factors.get(0);
        for (int i = 1; i < factors.size(); i++) {
            result = result.multiply(factors.get(i));
        }

        return result;
    }

    private Map<String, String> initializeSamplingState(Map<String, String> evidence) {
        Map<String, String> state = new HashMap<>(evidence);

        // 为非证据变量随机初始化
        Random random = new Random();
        for (String variableName : variables.keySet()) {
            if (!evidence.containsKey(variableName)) {
                Variable variable = variables.get(variableName);
                List<String> states = variable.getStates();
                String randomState = states.get(random.nextInt(states.size()));
                state.put(variableName, randomState);
            }
        }

        return state;
    }

    private String sampleVariable(String variable, Map<String, String> currentState,
                                Map<String, String> evidence) {
        List<String> parents = adjacencyList.get(variable);
        ConditionalProbabilityTable cpt = cpts.get(variable);

        Map<String, String> parentAssignment = new HashMap<>();
        for (String parent : parents) {
            parentAssignment.put(parent, currentState.get(parent));
        }

        // 计算条件概率
        Variable var = variables.get(variable);
        List<Double> probabilities = new ArrayList<>();

        for (String state : var.getStates()) {
            probabilities.add(cpt.getProbability(state, parentAssignment));
        }

        // 按概率采样
        Random random = new Random();
        double cumulative = 0.0;
        double rand = random.nextDouble();

        for (int i = 0; i < var.getStates().size(); i++) {
            cumulative += probabilities.get(i);
            if (rand <= cumulative) {
                return var.getStates().get(i);
            }
        }

        return var.getStates().get(var.getStates().size() - 1);
    }

    // 其他辅助方法...
    private String getParentConfiguration(Map<String, String> sample, List<String> parents) {
        if (parents.isEmpty()) {
            return "none";
        }

        StringBuilder config = new StringBuilder();
        for (String parent : parents) {
            config.append(sample.get(parent)).append(",");
        }
        return config.toString();
    }

    private Map<String, String> parseParentConfig(String config) {
        if ("none".equals(config)) {
            return Collections.emptyMap();
        }

        Map<String, String> result = new HashMap<>();
        String[] parts = config.split(",");
        // 简化实现，实际需要对应具体的父变量
        return result;
    }

    private List<String> getAllParentConfigurations(List<String> parents) {
        if (parents.isEmpty()) {
            return Collections.singletonList("none");
        }
        // 简化实现，实际需要生成所有可能的父变量配置组合
        return Collections.singletonList("default");
    }

    // 数据结构定义
    public static class Variable {
        private final String name;
        private final List<String> states;

        public Variable(String name, List<String> states) {
            this.name = name;
            this.states = new ArrayList<>(states);
        }

        public String getName() { return name; }
        public List<String> getStates() { return new ArrayList<>(states); }
    }

    public static class ConditionalProbabilityTable {
        private final String variable;
        private final List<String> parents;
        private final Map<String, Map<String, Double>> probabilities;

        public ConditionalProbabilityTable(String variable, List<String> parents) {
            this.variable = variable;
            this.parents = new ArrayList<>(parents);
            this.probabilities = new HashMap<>();
        }

        public void setProbability(String state, Map<String, String> parentConfig, double probability) {
            String key = createKey(state, parentConfig);
            probabilities.put(key, probability);
        }

        public double getProbability(String state, Map<String, String> parentConfig) {
            String key = createKey(state, parentConfig);
            return probabilities.getOrDefault(key, 0.0);
        }

        private String createKey(String state, Map<String, String> parentConfig) {
            StringBuilder key = new StringBuilder(state);
            for (String parent : parents) {
                key.append("|").append(parentConfig.getOrDefault(parent, "unknown"));
            }
            return key.toString();
        }
    }

    public static class Factor {
        private final List<String> variables;
        private final Map<Map<String, String>, Double> values;

        public Factor(List<String> variables) {
            this.variables = new ArrayList<>(variables);
            this.values = new HashMap<>();
        }

        public List<String> getVariables() { return new ArrayList<>(variables); }

        public double getValue(Map<String, String> assignment) {
            return values.getOrDefault(assignment, 0.0);
        }

        public Factor multiply(Factor other) {
            // 简化实现
            return this;
        }

        public Factor marginalize(String variable) {
            // 简化实现
            return this;
        }

        public void reduce(String variable, String value) {
            // 简化实现
        }
    }
}
```

---

**总结**: 概率论与统计学是机器学习的理论基础，从基础的贝叶斯分类到复杂的概率图模型，都需要扎实的概率统计知识。掌握这些理论并能在Java中实现，对于构建鲁棒和可解释的AI系统至关重要。