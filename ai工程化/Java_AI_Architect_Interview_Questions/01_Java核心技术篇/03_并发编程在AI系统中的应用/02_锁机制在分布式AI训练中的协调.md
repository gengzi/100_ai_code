# 锁机制在分布式AI训练中的协调 (140题)

## ⭐ 基础题 (1-42)

### 问题1: 分布式AI训练中的参数同步锁设计

**面试题**: 在分布式深度学习训练中，如何设计高效的参数更新锁机制避免冲突？

**口语化答案**:
"分布式训练中的参数同步是关键。我会设计分层的锁策略来优化性能：

```java
public class DistributedTrainingCoordinator {

    // 分层参数锁管理器
    public static class HierarchicalParameterLock {
        private final ConcurrentHashMap<String, ReadWriteLock> layerLocks;
        private final StampedLock globalLock;
        private final AtomicInteger lockUpgrades = new AtomicInteger(0);

        public HierarchicalParameterLock() {
            this.layerLocks = new ConcurrentHashMap<>();
            this.globalLock = new StampedLock();
        }

        // 参数读取 - 使用乐观读锁
        public double[] readParameters(String layerName, String parameterName) {
            ReadWriteLock layerLock = layerLocks.computeIfAbsent(layerName,
                k -> new ReentrantReadWriteLock());

            // 优先尝试乐观读
            long stamp = globalLock.tryOptimisticRead();
            layerLock.readLock().lock();
            try {
                // 验证乐观读是否有效
                if (!globalLock.validate(stamp)) {
                    stamp = globalLock.readLock();
                    try {
                        return readParameterFromStorage(layerName, parameterName);
                    } finally {
                        globalLock.unlockRead(stamp);
                    }
                }
                return readParameterFromStorage(layerName, parameterName);
            } finally {
                layerLock.readLock().unlock();
            }
        }

        // 参数更新 - 使用写锁
        public boolean updateParameters(String layerName, Map<String, double[]> gradients,
                                      double learningRate) {
            ReadWriteLock layerLock = layerLocks.computeIfAbsent(layerName,
                k -> new ReentrantReadWriteLock());

            long stamp = globalLock.writeLock();
            layerLock.writeLock().lock();
            try {
                // 检查是否有其他worker正在更新
                if (isParameterLocked(layerName)) {
                    return false; // 参数被锁定，无法更新
                }

                // 执行参数更新
                return applyParameterUpdate(layerName, gradients, learningRate);

            } finally {
                layerLock.writeLock().unlock();
                globalLock.unlockWrite(stamp);
            }
        }

        // 批量参数更新 - 减少锁竞争
        public CompletableFuture<Boolean> batchUpdateParameters(
                Map<String, Map<String, double[]>> batchGradients,
                double learningRate) {

            return CompletableFuture.supplyAsync(() -> {
                // 按层排序，避免死锁
                List<String> sortedLayers = new ArrayList<>(batchGradients.keySet());
                Collections.sort(sortedLayers);

                long stamp = globalLock.writeLock();
                try {
                    for (String layerName : sortedLayers) {
                        ReadWriteLock layerLock = layerLocks.computeIfAbsent(layerName,
                            k -> new ReentrantReadWriteLock());

                        layerLock.writeLock().lock();
                        try {
                            Map<String, double[]> layerGradients = batchGradients.get(layerName);
                            if (!applyParameterUpdate(layerName, layerGradients, learningRate)) {
                                return false;
                            }
                        } finally {
                            layerLock.writeLock().unlock();
                        }
                    }
                    return true;
                } finally {
                    globalLock.unlockWrite(stamp);
                }
            });
        }

        private boolean isParameterLocked(String layerName) {
            // 检查参数是否被其他worker锁定
            return false; // 简化实现
        }

        private boolean applyParameterUpdate(String layerName,
                                           Map<String, double[]> gradients,
                                           double learningRate) {
            // 实际的参数更新逻辑
            for (Map.Entry<String, double[]> entry : gradients.entrySet()) {
                double[] currentParams = readParameterFromStorage(layerName, entry.getKey());
                double[] grads = entry.getValue();

                for (int i = 0; i < currentParams.length; i++) {
                    currentParams[i] -= learningRate * grads[i];
                }

                writeParameterToStorage(layerName, entry.getKey(), currentParams);
            }
            return true;
        }

        private double[] readParameterFromStorage(String layerName, String parameterName) {
            // 模拟从参数服务器读取
            return new double[100]; // 简化实现
        }

        private void writeParameterToStorage(String layerName, String parameterName,
                                           double[] parameters) {
            // 模拟写入参数服务器
        }
    }

    // 分布式训练协调器
    public static class DistributedTrainingCoordinator {
        private final String workerId;
        private final HierarchicalParameterLock parameterLock;
        private final ExecutorService trainingExecutor;
        private final ConcurrentHashMap<String, AtomicLong> updateVersions;

        public DistributedTrainingCoordinator(String workerId, int numThreads) {
            this.workerId = workerId;
            this.parameterLock = new HierarchicalParameterLock();
            this.trainingExecutor = Executors.newFixedThreadPool(numThreads);
            this.updateVersions = new ConcurrentHashMap<>();
        }

        // 分布式训练主循环
        public CompletableFuture<Void> startTraining(TrainingConfig config,
                                                   DataSet trainingData) {

            List<CompletableFuture<Void>> epochTasks = new ArrayList<>();

            for (int epoch = 0; epoch < config.getEpochs(); epoch++) {
                CompletableFuture<Void> epochTask = runTrainingEpoch(epoch, trainingData, config);
                epochTasks.add(epochTask);
            }

            return CompletableFuture.allOf(epochTasks.toArray(new CompletableFuture[0]));
        }

        private CompletableFuture<Void> runTrainingEpoch(int epoch,
                                                       DataSet trainingData,
                                                       TrainingConfig config) {
            return CompletableFuture.runAsync(() -> {
                System.out.printf("Worker %s 开始第 %d 轮训练%n", workerId, epoch);

                List<DataBatch> batches = trainingData.getBatches();

                for (DataBatch batch : batches) {
                    processBatch(batch, config);
                }
            }, trainingExecutor);
        }

        private void processBatch(DataBatch batch, TrainingConfig config) {
            // 1. 前向传播
            Map<String, double[]> activations = forwardPass(batch);

            // 2. 反向传播计算梯度
            Map<String, Map<String, double[]>> gradients = backwardPass(batch, activations);

            // 3. 参数同步和更新
            synchronizeAndParametersUpdate(gradients, config.getLearningRate());
        }

        private Map<String, double[]> forwardPass(DataBatch batch) {
            Map<String, double[]> activations = new HashMap<>();

            // 简化的前向传播
            batch.getSamples().forEach(sample -> {
                activations.put("layer1", new double[256]);
                activations.put("layer2", new double[128]);
                activations.put("output", new double[10]);
            });

            return activations;
        }

        private Map<String, Map<String, double[]>> backwardPass(DataBatch batch,
                                                              Map<String, double[]> activations) {
            Map<String, Map<String, double[]>> gradients = new HashMap<>();

            // 简化的梯度计算
            gradients.put("layer1", Map.of(
                "weights", generateRandomGradients(784, 256),
                "bias", generateRandomGradients(256)
            ));

            gradients.put("layer2", Map.of(
                "weights", generateRandomGradients(256, 128),
                "bias", generateRandomGradients(128)
            ));

            gradients.put("output", Map.of(
                "weights", generateRandomGradients(128, 10),
                "bias", generateRandomGradients(10)
            ));

            return gradients;
        }

        private Map<String, double[]> generateRandomGradients(int... dimensions) {
            if (dimensions.length == 1) {
                double[] grads = new double[dimensions[0]];
                Random random = new Random();
                for (int i = 0; i < grads.length; i++) {
                    grads[i] = random.nextGaussian() * 0.001;
                }
                return Map.of("gradient", grads);
            } else {
                // 多维矩阵的简化表示
                return Map.of("gradient", new double[dimensions[0] * dimensions[1]]);
            }
        }

        private void synchronizeAndParametersUpdate(
                Map<String, Map<String, double[]>> gradients,
                double learningRate) {

            // 使用分布式锁协调参数更新
            boolean success = parameterLock.batchUpdateParameters(gradients, learningRate)
                .join();

            if (success) {
                // 记录更新版本
                for (String layerName : gradients.keySet()) {
                    updateVersions.compute(layerName, (k, v) -> v == null ? 1 : v + 1);
                }
            } else {
                System.out.printf("Worker %s 参数更新失败，尝试重试%n", workerId);
                // 可以实现重试逻辑
            }
        }

        // 获取训练统计信息
        public TrainingStats getTrainingStats() {
            return new TrainingStats(
                workerId,
                updateVersions.size(),
                updateVersions.values().stream().mapToLong(Long::longValue).sum()
            );
        }

        public void shutdown() {
            trainingExecutor.shutdown();
        }
    }

    // 训练配置
    public static class TrainingConfig {
        private final int epochs;
        private final double learningRate;
        private final int batchSize;

        public TrainingConfig(int epochs, double learningRate, int batchSize) {
            this.epochs = epochs;
            this.learningRate = learningRate;
            this.batchSize = batchSize;
        }

        // getters...
        public int getEpochs() { return epochs; }
        public double getLearningRate() { return learningRate; }
        public int getBatchSize() { return batchSize; }
    }

    // 数据批次
    public static class DataBatch {
        private final String batchId;
        private final List<SampleData> samples;

        public DataBatch(String batchId, List<SampleData> samples) {
            this.batchId = batchId;
            this.samples = samples;
        }

        // getters...
        public String getBatchId() { return batchId; }
        public List<SampleData> getSamples() { return samples; }
    }

    // 样本数据
    public static class SampleData {
        private final double[] features;
        private final double[] labels;

        public SampleData(double[] features, double[] labels) {
            this.features = features;
            this.labels = labels;
        }

        // getters...
        public double[] getFeatures() { return features; }
        public double[] getLabels() { return labels; }
    }

    // 数据集
    public static class DataSet {
        private final List<DataBatch> batches;

        public DataSet(List<DataBatch> batches) {
            this.batches = batches;
        }

        public List<DataBatch> getBatches() { return batches; }
    }

    // 训练统计
    public static class TrainingStats {
        private final String workerId;
        private final int updatedLayers;
        private final long totalUpdates;

        public TrainingStats(String workerId, int updatedLayers, long totalUpdates) {
            this.workerId = workerId;
            this.updatedLayers = updatedLayers;
            this.totalUpdates = totalUpdates;
        }

        @Override
        public String toString() {
            return String.format("Worker %s: 更新了 %d 层，总更新次数 %d",
                workerId, updatedLayers, totalUpdates);
        }
    }
}
```

## ⭐⭐ 进阶题 (43-98)

### 问题43: 无锁算法在分布式梯度聚合中的应用

**面试题**: 如何使用CAS操作实现无锁的分布式梯度聚合，提高训练效率？

**口语化答案**:
"无锁算法能显著提升分布式训练性能。我会用CAS和原子操作来实现：

```java
public class LockFreeGradientAggregator {

    // 无锁梯度聚合器
    public static class LockFreeAggregator {
        private final AtomicReference<GradientAccumulator> accumulator;
        private final AtomicInteger activeWorkers;
        private final CompletableFuture<AggregationResult> aggregationFuture;
        private final int totalWorkers;

        public LockFreeAggregator(int totalWorkers) {
            this.totalWorkers = totalWorkers;
            this.activeWorkers = new AtomicInteger(totalWorkers);
            this.accumulator = new AtomicReference<>(new GradientAccumulator());
            this.aggregationFuture = new CompletableFuture<>();
        }

        // 工作节点提交梯度
        public boolean submitGradient(String workerId, Map<String, double[]> gradients) {
            GradientAccumulator current;
            GradientAccumulator newAccumulator;

            do {
                current = accumulator.get();
                newAccumulator = current.copy();

                // 合并梯度
                newAccumulator.mergeGradients(gradients, workerId);
                newAccumulator.incrementWorkerCount();

            } while (!accumulator.compareAndSet(current, newAccumulator));

            // 检查是否所有worker都提交了梯度
            int remainingWorkers = activeWorkers.decrementAndGet();
            if (remainingWorkers == 0) {
                completeAggregation();
            }

            return true;
        }

        private void completeAggregation() {
            GradientAccumulator finalAccumulator = accumulator.get();

            // 计算平均梯度
            Map<String, double[]> averagedGradients = finalAccumulator.computeAverageGradients();

            // 完成聚合
            AggregationResult result = new AggregationResult(
                true,
                averagedGradients,
                finalAccumulator.getParticipatingWorkers(),
                System.currentTimeMillis()
            );

            aggregationFuture.complete(result);
        }

        public CompletableFuture<AggregationResult> getAggregationResult() {
            return aggregationFuture;
        }

        public void reset() {
            activeWorkers.set(totalWorkers);
            accumulator.set(new GradientAccumulator());
        }
    }

    // 梯度累加器
    public static class GradientAccumulator {
        private final ConcurrentHashMap<String, AtomicDoubleArray> gradients;
        private final AtomicInteger workerCount;
        private final Set<String> participatingWorkers;

        public GradientAccumulator() {
            this.gradients = new ConcurrentHashMap<>();
            this.workerCount = new AtomicInteger(0);
            this.participatingWorkers = ConcurrentHashMap.newKeySet();
        }

        public GradientAccumulator copy() {
            GradientAccumulator copy = new GradientAccumulator();
            copy.workerCount.set(workerCount.get());
            copy.participatingWorkers.addAll(participatingWorkers);

            // 深拷贝梯度数据
            gradients.forEach((key, gradArray) -> {
                double[] copyArray = new double[gradArray.length()];
                for (int i = 0; i < copyArray.length; i++) {
                    copyArray[i] = gradArray.get(i);
                }
                copy.gradients.put(key, new AtomicDoubleArray(copyArray));
            });

            return copy;
        }

        public void mergeGradients(Map<String, double[]> newGradients, String workerId) {
            participatingWorkers.add(workerId);

            newGradients.forEach((paramName, grads) -> {
                AtomicDoubleArray existing = gradients.computeIfAbsent(paramName,
                    k -> new AtomicDoubleArray(grads.length));

                // 使用CAS操作原子累加梯度
                for (int i = 0; i < grads.length; i++) {
                    double oldValue, newValue;
                    do {
                        oldValue = existing.get(i);
                        newValue = oldValue + grads[i];
                    } while (!existing.compareAndSet(i, oldValue, newValue));
                }
            });
        }

        public void incrementWorkerCount() {
            workerCount.incrementAndGet();
        }

        public Map<String, double[]> computeAverageGradients() {
            int numWorkers = workerCount.get();
            Map<String, double[]> averagedGradients = new HashMap<>();

            gradients.forEach((paramName, gradArray) -> {
                double[] averaged = new double[gradArray.length()];
                for (int i = 0; i < averaged.length; i++) {
                    averaged[i] = gradArray.get(i) / numWorkers;
                }
                averagedGradients.put(paramName, averaged);
            });

            return averagedGradients;
        }

        // getters...
        public int getWorkerCount() { return workerCount.get(); }
        public Set<String> getParticipatingWorkers() { return new HashSet<>(participatingWorkers); }
    }

    // 聚合结果
    public static class AggregationResult {
        private final boolean success;
        private final Map<String, double[]> averagedGradients;
        private final Set<String> participatingWorkers;
        private final long timestamp;

        public AggregationResult(boolean success, Map<String, double[]> averagedGradients,
                                Set<String> participatingWorkers, long timestamp) {
            this.success = success;
            this.averagedGradients = averagedGradients;
            this.participatingWorkers = participatingWorkers;
            this.timestamp = timestamp;
        }

        // getters...
        public boolean isSuccess() { return success; }
        public Map<String, double[]> getAveragedGradients() { return averagedGradients; }
        public Set<String> getParticipatingWorkers() { return participatingWorkers; }
        public long getTimestamp() { return timestamp; }
    }

    // 分布式训练协调器（无锁版本）
    public static class LockFreeDistributedTrainer {
        private final String workerId;
        private final LockFreeAggregator aggregator;
        private final ExecutorService trainingExecutor;
        private final ParameterServer parameterServer;

        public LockFreeDistributedTrainer(String workerId, int numWorkers) {
            this.workerId = workerId;
            this.aggregator = new LockFreeAggregator(numWorkers);
            this.trainingExecutor = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors());
            this.parameterServer = new ParameterServer();
        }

        // 开始分布式训练
        public CompletableFuture<Void> startTraining(TrainingDataset dataset,
                                                   int epochs) {
            return CompletableFuture.runAsync(() -> {
                for (int epoch = 0; epoch < epochs; epoch++) {
                    trainEpoch(dataset, epoch);
                }
            }, trainingExecutor);
        }

        private void trainEpoch(TrainingDataset dataset, int epoch) {
            List<DataBatch> batches = dataset.getBatches();

            // 并行处理所有批次
            List<CompletableFuture<LocalGradients>> batchTasks = batches.stream()
                .map(batch -> CompletableFuture.supplyAsync(() -> {
                    return computeLocalGradients(batch);
                }, trainingExecutor))
                .collect(Collectors.toList());

            // 等待所有批次完成并聚合梯度
            CompletableFuture<Void> allBatches = CompletableFuture.allOf(
                batchTasks.toArray(new CompletableFuture[0]));

            allBatches.thenRun(() -> {
                // 合并所有批次的局部梯度
                Map<String, double[]> totalGradients = batchTasks.stream()
                    .map(CompletableFuture::join)
                    .reduce(new HashMap<>(), this::mergeLocalGradients);

                // 提交到全局聚合器
                boolean submitted = aggregator.submitGradient(workerId, totalGradients);

                if (submitted) {
                    // 等待全局聚合完成
                    aggregator.getAggregationResult().thenAccept(result -> {
                        if (result.isSuccess()) {
                            // 应用平均梯度更新参数
                            parameterServer.updateParameters(result.getAveragedGradients(), 0.01);
                            aggregator.reset(); // 重置聚合器准备下一轮
                        }
                    });
                }
            }).join();
        }

        private LocalGradients computeLocalGradients(DataBatch batch) {
            // 模拟局部梯度计算
            Map<String, double[]> gradients = new HashMap<>();

            // 为每个层计算梯度
            gradients.put("layer1.weights", generateRandomGradient(784 * 256));
            gradients.put("layer1.bias", generateRandomGradient(256));
            gradients.put("layer2.weights", generateRandomGradient(256 * 128));
            gradients.put("layer2.bias", generateRandomGradient(128));

            return new LocalGradients(batch.getBatchId(), gradients);
        }

        private Map<String, double[]> mergeLocalGradients(Map<String, double[]> acc,
                                                        LocalGradients local) {
            Map<String, double[]> result = new HashMap<>(acc);
            Map<String, double[]> localGrads = local.getGradients();

            localGrads.forEach((key, grads) -> {
                result.merge(key, grads, (existing, newGrads) -> {
                    double[] merged = new double[existing.length];
                    for (int i = 0; i < merged.length; i++) {
                        merged[i] = existing[i] + newGrads[i];
                    }
                    return merged;
                });
            });

            return result;
        }

        private double[] generateRandomGradient(int size) {
            double[] grads = new double[size];
            Random random = new Random();
            for (int i = 0; i < size; i++) {
                grads[i] = random.nextGaussian() * 0.001;
            }
            return grads;
        }

        // 局部梯度结果
        public static class LocalGradients {
            private final String batchId;
            private final Map<String, double[]> gradients;

            public LocalGradients(String batchId, Map<String, double[]> gradients) {
                this.batchId = batchId;
                this.gradients = gradients;
            }

            // getters...
            public String getBatchId() { return batchId; }
            public Map<String, double[]> getGradients() { return gradients; }
        }

        // 参数服务器
        public static class ParameterServer {
            private final ConcurrentHashMap<String, double[]> parameters;

            public ParameterServer() {
                this.parameters = new ConcurrentHashMap<>();
                initializeParameters();
            }

            private void initializeParameters() {
                parameters.put("layer1.weights", new double[784 * 256]);
                parameters.put("layer1.bias", new double[256]);
                parameters.put("layer2.weights", new double[256 * 128]);
                parameters.put("layer2.bias", new double[128]);

                // 随机初始化
                Random random = new Random(42);
                parameters.values().forEach(paramArray -> {
                    for (int i = 0; i < paramArray.length; i++) {
                        paramArray[i] = random.nextGaussian() * 0.01;
                    }
                });
            }

            public void updateParameters(Map<String, double[]> gradients, double learningRate) {
                gradients.forEach((paramName, grads) -> {
                    double[] params = parameters.get(paramName);
                    if (params != null) {
                        // 原子更新参数
                        for (int i = 0; i < Math.min(params.length, grads.length); i++) {
                            params[i] -= learningRate * grads[i];
                        }
                    }
                });
            }

            public Map<String, double[]> getParameters() {
                return new HashMap<>(parameters);
            }
        }

        public void shutdown() {
            trainingExecutor.shutdown();
        }
    }

    // 训练数据集
    public static class TrainingDataset {
        private final List<DataBatch> batches;

        public TrainingDataset(List<DataBatch> batches) {
            this.batches = batches;
        }

        public List<DataBatch> getBatches() { return batches; }
    }

    // 数据批次
    public static class DataBatch {
        private final String batchId;
        private final List<TrainingSample> samples;

        public DataBatch(String batchId, List<TrainingSample> samples) {
            this.batchId = batchId;
            this.samples = samples;
        }

        // getters...
        public String getBatchId() { return batchId; }
        public List<TrainingSample> getSamples() { return samples; }
    }

    // 训练样本
    public static class TrainingSample {
        private final double[] features;
        private final int label;

        public TrainingSample(double[] features, int label) {
            this.features = features;
            this.label = label;
        }

        // getters...
        public double[] getFeatures() { return features; }
        public int getLabel() { return label; }
    }
}
```

## ⭐⭐⭐ 专家题 (99-140)

### 问题99: 基于Raft的分布式训练一致性保证

**面试题**: 如何实现基于Raft共识算法的分布式AI训练参数一致性？

**口语化答案**:
"Raft能保证分布式训练的强一致性。我会实现一个基于Raft的参数服务器：

```java
public class RaftBasedParameterServer {

    // Raft节点
    public static class RaftNode {
        private final String nodeId;
        private final RaftState state;
        private final RaftLog raftLog;
        private final ParameterStore parameterStore;
        private final NetworkLayer network;
        private final ScheduledExecutorService scheduler;

        public RaftNode(String nodeId, List<String> allNodes) {
            this.nodeId = nodeId;
            this.state = new RaftState(nodeId, allNodes.size());
            this.raftLog = new RaftLog();
            this.parameterStore = new ParameterStore();
            this.network = new NetworkLayer(nodeId, allNodes);
            this.scheduler = Executors.newSingleThreadScheduledExecutor();

            startRaftProtocol();
        }

        // 启动Raft协议
        private void startRaftProtocol() {
            // 定期选举超时
            scheduler.scheduleAtFixedRate(() -> {
                if (state.getRole() == RaftRole.LEADER) {
                    sendHeartbeats();
                } else if (state.getRole() == RaftRole.FOLLOWER) {
                    checkElectionTimeout();
                }
            }, 0, 50, TimeUnit.MILLISECONDS);
        }

        // 处理参数更新请求
        public CompletableFuture<UpdateResult> updateParameters(
                Map<String, double[]> gradients,
                double learningRate) {

            if (state.getRole() != RaftRole.LEADER) {
                // 转发给leader
                return forwardToLeader(gradients, learningRate);
            }

            // Leader处理更新
            return leaderUpdateParameters(gradients, learningRate);
        }

        private CompletableFuture<UpdateResult> leaderUpdateParameters(
                Map<String, double[]> gradients,
                double learningRate) {

            ParameterUpdateEntry entry = new ParameterUpdateEntry(
                gradients, learningRate, System.currentTimeMillis());

            // 添加到Raft日志
            long logIndex = raftLog.appendEntry(entry);

            // 复制到followers
            List<CompletableFuture<Boolean>> replicationResults = new ArrayList<>();
            for (String followerId : state.getOtherNodes()) {
                replicationResults.add(
                    replicateToFollower(followerId, logIndex, entry)
                );
            }

            // 等待大多数节点确认
            return CompletableFuture.allOf(replicationResults.toArray(new CompletableFuture[0]))
                .thenApply(v -> {
                    // 检查是否获得多数确认
                    long successes = replicationResults.stream()
                        .mapToLong(future -> future.join() ? 1 : 0)
                        .sum();

                    if (successes >= state.getMajoritySize()) {
                        // 提交到状态机
                        commitToStateMachine(logIndex);
                        return new UpdateResult(true, "参数更新成功", logIndex);
                    } else {
                        return new UpdateResult(false, "未获得多数确认", -1);
                    }
                });
        }

        private CompletableFuture<Boolean> replicateToFollower(String followerId,
                                                             long logIndex,
                                                             ParameterUpdateEntry entry) {
            AppendEntriesRequest request = new AppendEntriesRequest(
                state.getCurrentTerm(),
                nodeId,
                raftLog.getPrevLogIndex(followerId),
                raftLog.getPrevLogTerm(followerId),
                List.of(entry),
                raftLog.getCommitIndex()
            );

            return network.sendAppendEntries(followerId, request)
                .thenApply(response -> response.isSuccess());
        }

        private void commitToStateMachine(long logIndex) {
            RaftLogEntry entry = raftLog.getEntry(logIndex);
            if (entry instanceof ParameterUpdateEntry) {
                ParameterUpdateEntry updateEntry = (ParameterUpdateEntry) entry;
                parameterStore.applyUpdate(updateEntry.getGradients(),
                                         updateEntry.getLearningRate());
                raftLog.setCommitIndex(logIndex);
            }
        }

        private CompletableFuture<UpdateResult> forwardToLeader(
                Map<String, double[]> gradients, double learningRate) {

            String leaderId = state.getCurrentLeader();
            if (leaderId == null) {
                return CompletableFuture.completedFuture(
                    new UpdateResult(false, "当前无leader", -1)
                );
            }

            return network.forwardParameterUpdate(leaderId, gradients, learningRate);
        }

        private void sendHeartbeats() {
            AppendEntriesRequest heartbeat = new AppendEntriesRequest(
                state.getCurrentTerm(),
                nodeId,
                raftLog.getPrevLogIndex(null), // heartbeat
                0,
                Collections.emptyList(),
                raftLog.getCommitIndex()
            );

            for (String followerId : state.getOtherNodes()) {
                network.sendAppendEntries(followerId, heartbeat);
            }
        }

        private void checkElectionTimeout() {
            if (System.currentTimeMillis() - state.getLastHeartbeatTime() > state.getElectionTimeout()) {
                startElection();
            }
        }

        private void startElection() {
            state.becomeCandidate();
            long newTerm = state.incrementTerm();

            VoteRequest voteRequest = new VoteRequest(newTerm, nodeId, raftLog.getLastLogIndex(),
                                                  raftLog.getLastLogTerm());

            List<CompletableFuture<VoteResponse>> voteResponses = new ArrayList<>();
            for (String nodeId : state.getOtherNodes()) {
                voteResponses.add(network.requestVote(nodeId, voteRequest));
            }

            // 给自己投票
            AtomicInteger votesGranted = new AtomicInteger(1);

            CompletableFuture<Void> electionResult = CompletableFuture.allOf(
                voteResponses.toArray(new CompletableFuture[0]));

            electionResult.thenRun(() -> {
                long votes = voteResponses.stream()
                    .mapToLong(future -> future.join().isVoteGranted() ? 1 : 0)
                    .sum() + votesGranted.get();

                if (votes >= state.getMajoritySize() && state.getRole() == RaftRole.CANDIDATE) {
                    becomeLeader();
                }
            });
        }

        private void becomeLeader() {
            state.becomeLeader(nodeId);
            System.out.println("节点 " + nodeId + " 成为Leader");
        }

        // 处理网络消息
        public void handleAppendEntries(AppendEntriesRequest request) {
            AppendEntriesResponse response = new AppendEntriesResponse(
                state.getCurrentTerm(),
                false
            );

            if (request.getTerm() < state.getCurrentTerm()) {
                network.sendResponse(request.getLeaderId(), response);
                return;
            }

            if (request.getTerm() > state.getCurrentTerm()) {
                state.setCurrentTerm(request.getTerm());
                state.becomeFollower(request.getLeaderId());
            }

            state.setLastHeartbeatTime(System.currentTimeMillis());

            // 验证日志一致性
            if (raftLog.isConsistent(request.getPrevLogIndex(), request.getPrevLogTerm())) {
                if (!request.getEntries().isEmpty()) {
                    raftLog.appendEntries(request.getPrevLogIndex() + 1, request.getEntries());
                }

                if (request.getLeaderCommit() > raftLog.getCommitIndex()) {
                    raftLog.setCommitIndex(Math.min(request.getLeaderCommit(),
                                                  raftLog.getLastLogIndex()));
                    // 应用已提交的条目到状态机
                    applyCommittedEntries();
                }

                response = new AppendEntriesResponse(state.getCurrentTerm(), true);
            }

            network.sendResponse(request.getLeaderId(), response);
        }

        private void applyCommittedEntries() {
            while (raftLog.getCommitIndex() > raftLog.getLastApplied()) {
                long index = raftLog.getLastApplied() + 1;
                RaftLogEntry entry = raftLog.getEntry(index);
                if (entry instanceof ParameterUpdateEntry) {
                    ParameterUpdateEntry updateEntry = (ParameterUpdateEntry) entry;
                    parameterStore.applyUpdate(updateEntry.getGradients(),
                                             updateEntry.getLearningRate());
                }
                raftLog.setLastApplied(index);
            }
        }

        public RaftState getState() { return state; }
        public ParameterStore getParameterStore() { return parameterStore; }

        public void shutdown() {
            scheduler.shutdown();
            network.shutdown();
        }
    }

    // Raft状态
    public static class RaftState {
        private volatile RaftRole role;
        private final String nodeId;
        private final int clusterSize;
        private volatile String currentLeader;
        private volatile long currentTerm;
        private volatile long lastHeartbeatTime;
        private final long electionTimeout;
        private final List<String> otherNodes;

        public RaftState(String nodeId, int clusterSize) {
            this.nodeId = nodeId;
            this.clusterSize = clusterSize;
            this.role = RaftRole.FOLLOWER;
            this.currentTerm = 0;
            this.lastHeartbeatTime = System.currentTimeMillis();
            this.electionTimeout = 150 + ThreadLocalRandom.current().nextInt(150);
            this.otherNodes = new ArrayList<>();
        }

        public void setOtherNodes(List<String> allNodes) {
            otherNodes.clear();
            otherNodes.addAll(allNodes);
            otherNodes.remove(nodeId);
        }

        public List<String> getOtherNodes() { return new ArrayList<>(otherNodes); }

        public int getMajoritySize() {
            return clusterSize / 2 + 1;
        }

        // 状态转换方法
        public void becomeFollower(String leaderId) {
            this.role = RaftRole.FOLLOWER;
            this.currentLeader = leaderId;
        }

        public void becomeCandidate() {
            this.role = RaftRole.CANDIDATE;
            this.currentLeader = null;
            this.lastHeartbeatTime = System.currentTimeMillis();
        }

        public void becomeLeader(String nodeId) {
            this.role = RaftRole.LEADER;
            this.currentLeader = nodeId;
        }

        // getters and setters...
        public RaftRole getRole() { return role; }
        public String getCurrentLeader() { return currentLeader; }
        public long getCurrentTerm() { return currentTerm; }
        public void setCurrentTerm(long term) { this.currentTerm = term; }
        public long incrementTerm() { return ++this.currentTerm; }
        public long getLastHeartbeatTime() { return lastHeartbeatTime; }
        public void setLastHeartbeatTime(long time) { this.lastHeartbeatTime = time; }
        public long getElectionTimeout() { return electionTimeout; }
    }

    // Raft角色枚举
    public enum RaftRole {
        FOLLOWER, CANDIDATE, LEADER
    }

    // Raft日志
    public static class RaftLog {
        private final List<RaftLogEntry> entries;
        private volatile long commitIndex;
        private volatile long lastApplied;

        public RaftLog() {
            this.entries = new ArrayList<>();
            this.commitIndex = 0;
            this.lastApplied = 0;
        }

        public long appendEntry(RaftLogEntry entry) {
            entries.add(entry);
            return entries.size(); // 返回日志索引
        }

        public void appendEntries(long startIndex, List<RaftLogEntry> newEntries) {
            // 移除冲突的条目并添加新条目
            if (startIndex <= entries.size()) {
                entries.subList((int) (startIndex - 1), entries.size()).clear();
            }
            entries.addAll(newEntries);
        }

        public boolean isConsistent(long prevLogIndex, long prevLogTerm) {
            if (prevLogIndex == 0) {
                return true; // 第一个条目总是一致的
            }

            if (prevLogIndex > entries.size()) {
                return false;
            }

            RaftLogEntry entry = entries.get((int) (prevLogIndex - 1));
            return entry.getTerm() == prevLogTerm;
        }

        public long getPrevLogIndex(String followerId) {
            // 简化实现，实际中需要为每个follower跟踪nextIndex
            return entries.size();
        }

        public long getPrevLogTerm(String followerId) {
            long index = getPrevLogIndex(followerId);
            if (index == 0) return 0;
            return entries.get((int) (index - 1)).getTerm();
        }

        public RaftLogEntry getEntry(long index) {
            if (index == 0 || index > entries.size()) {
                return null;
            }
            return entries.get((int) (index - 1));
        }

        public long getLastLogIndex() {
            return entries.size();
        }

        public long getLastLogTerm() {
            if (entries.isEmpty()) {
                return 0;
            }
            return entries.get(entries.size() - 1).getTerm();
        }

        // getters and setters...
        public long getCommitIndex() { return commitIndex; }
        public void setCommitIndex(long commitIndex) { this.commitIndex = commitIndex; }
        public long getLastApplied() { return lastApplied; }
        public void setLastApplied(long lastApplied) { this.lastApplied = lastApplied; }
    }

    // Raft日志条目接口
    public interface RaftLogEntry {
        long getTerm();
    }

    // 参数更新日志条目
    public static class ParameterUpdateEntry implements RaftLogEntry {
        private final long term;
        private final Map<String, double[]> gradients;
        private final double learningRate;
        private final long timestamp;

        public ParameterUpdateEntry(Map<String, double[]> gradients,
                                   double learningRate, long timestamp) {
            this.term = 0; // 在添加到日志时设置
            this.gradients = new HashMap<>(gradients);
            this.learningRate = learningRate;
            this.timestamp = timestamp;
        }

        public void setTerm(long term) {
            // 通过反射或其他方式设置term
        }

        // getters...
        @Override
        public long getTerm() { return term; }
        public Map<String, double[]> getGradients() { return gradients; }
        public double getLearningRate() { return learningRate; }
        public long getTimestamp() { return timestamp; }
    }

    // 参数存储
    public static class ParameterStore {
        private final ConcurrentHashMap<String, double[]> parameters;

        public ParameterStore() {
            this.parameters = new ConcurrentHashMap<>();
            initializeParameters();
        }

        private void initializeParameters() {
            // 初始化神经网络参数
            parameters.put("conv1.weights", new double[3 * 3 * 3 * 64]);
            parameters.put("conv1.bias", new double[64]);
            parameters.put("fc1.weights", new double[9216 * 128]);
            parameters.put("fc1.bias", new double[128]);
            parameters.put("fc2.weights", new double[128 * 10]);
            parameters.put("fc2.bias", new double[10]);

            // Xavier初始化
            Random random = new Random(42);
            parameters.values().forEach(paramArray -> {
                for (int i = 0; i < paramArray.length; i++) {
                    paramArray[i] = random.nextGaussian() * Math.sqrt(2.0 / paramArray.length);
                }
            });
        }

        public void applyUpdate(Map<String, double[]> gradients, double learningRate) {
            gradients.forEach((paramName, grads) -> {
                double[] params = parameters.get(paramName);
                if (params != null && grads.length == params.length) {
                    // 原子更新参数
                    for (int i = 0; i < params.length; i++) {
                        params[i] -= learningRate * grads[i];
                    }
                }
            });
        }

        public Map<String, double[]> getParameters() {
            Map<String, double[]> snapshot = new HashMap<>();
            parameters.forEach((key, value) -> snapshot.put(key, value.clone()));
            return snapshot;
        }
    }

    // 网络层（简化实现）
    public static class NetworkLayer {
        private final String nodeId;
        private final Map<String, RaftNode> clusterNodes;
        private final ExecutorService networkExecutor;

        public NetworkLayer(String nodeId, List<String> allNodes) {
            this.nodeId = nodeId;
            this.clusterNodes = new ConcurrentHashMap<>();
            this.networkExecutor = Executors.newFixedThreadPool(4);
            // 实际中这里应该建立网络连接
        }

        public CompletableFuture<AppendEntriesResponse> sendAppendEntries(
                String targetId, AppendEntriesRequest request) {
            return CompletableFuture.supplyAsync(() -> {
                RaftNode targetNode = clusterNodes.get(targetId);
                if (targetNode != null) {
                    targetNode.handleAppendEntries(request);
                    return new AppendEntriesResponse(0, true);
                }
                return new AppendEntriesResponse(0, false);
            }, networkExecutor);
        }

        public CompletableFuture<VoteResponse> requestVote(String targetId, VoteRequest request) {
            return CompletableFuture.supplyAsync(() -> {
                // 模拟投票响应
                return new VoteResponse(0, Math.random() > 0.3);
            }, networkExecutor);
        }

        public CompletableFuture<UpdateResult> forwardParameterUpdate(
                String leaderId, Map<String, double[]> gradients, double learningRate) {
            RaftNode leaderNode = clusterNodes.get(leaderId);
            if (leaderNode != null) {
                return leaderNode.updateParameters(gradients, learningRate);
            }
            return CompletableFuture.completedFuture(
                new UpdateResult(false, "Leader不可用", -1)
            );
        }

        public void sendResponse(String targetId, Object response) {
            // 模拟发送响应
        }

        public void shutdown() {
            networkExecutor.shutdown();
        }
    }

    // Raft消息类型
    public static class AppendEntriesRequest {
        private final long term;
        private final String leaderId;
        private final long prevLogIndex;
        private final long prevLogTerm;
        private final List<RaftLogEntry> entries;
        private final long leaderCommit;

        public AppendEntriesRequest(long term, String leaderId, long prevLogIndex,
                                   long prevLogTerm, List<RaftLogEntry> entries, long leaderCommit) {
            this.term = term;
            this.leaderId = leaderId;
            this.prevLogIndex = prevLogIndex;
            this.prevLogTerm = prevLogTerm;
            this.entries = entries;
            this.leaderCommit = leaderCommit;
        }

        // getters...
        public long getTerm() { return term; }
        public String getLeaderId() { return leaderId; }
        public long getPrevLogIndex() { return prevLogIndex; }
        public long getPrevLogTerm() { return prevLogTerm; }
        public List<RaftLogEntry> getEntries() { return entries; }
        public long getLeaderCommit() { return leaderCommit; }
    }

    public static class AppendEntriesResponse {
        private final long term;
        private final boolean success;

        public AppendEntriesResponse(long term, boolean success) {
            this.term = term;
            this.success = success;
        }

        // getters...
        public long getTerm() { return term; }
        public boolean isSuccess() { return success; }
    }

    public static class VoteRequest {
        private final long term;
        private final String candidateId;
        private final long lastLogIndex;
        private final long lastLogTerm;

        public VoteRequest(long term, String candidateId, long lastLogIndex, long lastLogTerm) {
            this.term = term;
            this.candidateId = candidateId;
            this.lastLogIndex = lastLogIndex;
            this.lastLogTerm = lastLogTerm;
        }

        // getters...
        public long getTerm() { return term; }
        public String getCandidateId() { return candidateId; }
        public long getLastLogIndex() { return lastLogIndex; }
        public long getLastLogTerm() { return lastLogTerm; }
    }

    public static class VoteResponse {
        private final long term;
        private final boolean voteGranted;

        public VoteResponse(long term, boolean voteGranted) {
            this.term = term;
            this.voteGranted = voteGranted;
        }

        // getters...
        public long getTerm() { return term; }
        public boolean isVoteGranted() { return voteGranted; }
    }

    // 更新结果
    public static class UpdateResult {
        private final boolean success;
        private final String message;
        private final long logIndex;

        public UpdateResult(boolean success, String message, long logIndex) {
            this.success = success;
            this.message = message;
            this.logIndex = logIndex;
        }

        // getters...
        public boolean isSuccess() { return success; }
        public String getMessage() { return message; }
        public long getLogIndex() { return logIndex; }
    }
}
```

## 💡 面试技巧提示

### 锁机制面试要点：

1. **分层锁设计**: 读写锁、StampedLock的合理使用
2. **无锁算法**: CAS、原子操作在梯度聚合中的应用
3. **分布式一致性**: Raft、Paxos在参数同步中的实现
4. **性能优化**: 减少锁竞争、提高并发度
5. **死锁预防**: 顺序加锁、超时机制

### 常见错误：
- 不了解分布式锁的实现原理
- 忽略锁的粒度设计和性能影响
- 缺乏分布式共识算法的理解
- 没有考虑网络分区和故障恢复
- 不了解现代并发编程的新特性

通过这些题目，面试官能全面考察候选人对并发编程和分布式系统设计的深度理解。